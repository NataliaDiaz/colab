{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Memorize with LSTM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "u0WhFqjqQksJ",
        "OpJjtoD5QuSO"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NataliaDiaz/colab/blob/master/Memorize_with_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "H7FQbW60N0Ky",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Memorize short sequences of letters with an LSTM\n",
        "\n",
        "The goal of this test is to use an LSTM to memorize a short sequence of letters and have the network produce that sequence when given the first letter of the sequence with 100% accuracy.\n",
        "\n",
        "In addition we want to do this in a strictly *online* setting. So each character is fed to the network one at a time, the network produces an output for each input, and we take a gradient step after each output is produced.\n",
        "\n",
        "## Questions\n",
        "\n",
        "Below we show that an LSTM is able to memorize and reproduce *some* short strings easily but not others. \n",
        "\n",
        "\n",
        "*   Why are some strings much harder to predict than others?\n",
        "*   Why does increasing the network size help in case 7?\n",
        "*   What are the underlying scaling factors in terms of memorization difficulty, capacity, and speed?\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "bkjJZsohInlP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import six\n",
        "import numpy as np\n",
        "from difflib import SequenceMatcher\n",
        "from plotly.offline import download_plotlyjs, init_notebook_mode\n",
        "from plotly.offline import iplot as plot\n",
        "from torch import nn\n",
        "\n",
        "torch.set_printoptions(precision=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u0WhFqjqQksJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Lib"
      ]
    },
    {
      "metadata": {
        "id": "0kAfku9HPtdn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# https://github.com/keras-team/keras-preprocessing/blob/master/keras_preprocessing/sequence.py#L15\n",
        "def pad_sequences(sequences, maxlen=None, dtype='int32', padding='pre', truncating='pre', value=0.):\n",
        "    \"\"\"Pads sequences to the same length.\n",
        "    This function transforms a list of\n",
        "    `num_samples` sequences (lists of integers)\n",
        "    into a 2D Numpy array of shape `(num_samples, num_timesteps)`.\n",
        "    `num_timesteps` is either the `maxlen` argument if provided,\n",
        "    or the length of the longest sequence otherwise.\n",
        "    Sequences that are shorter than `num_timesteps`\n",
        "    are padded with `value` at the end.\n",
        "    Sequences longer than `num_timesteps` are truncated\n",
        "    so that they fit the desired length.\n",
        "    The position where padding or truncation happens is determined by\n",
        "    the arguments `padding` and `truncating`, respectively.\n",
        "    Pre-padding is the default.\n",
        "    # Arguments\n",
        "        sequences: List of lists, where each element is a sequence.\n",
        "        maxlen: Int, maximum length of all sequences.\n",
        "        dtype: Type of the output sequences.\n",
        "            To pad sequences with variable length strings, you can use `object`.\n",
        "        padding: String, 'pre' or 'post':\n",
        "            pad either before or after each sequence.\n",
        "        truncating: String, 'pre' or 'post':\n",
        "            remove values from sequences larger than\n",
        "            `maxlen`, either at the beginning or at the end of the sequences.\n",
        "        value: Float or String, padding value.\n",
        "    # Returns\n",
        "        x: Numpy array with shape `(len(sequences), maxlen)`\n",
        "    # Raises\n",
        "        ValueError: In case of invalid values for `truncating` or `padding`,\n",
        "            or in case of invalid shape for a `sequences` entry.\n",
        "    \"\"\"\n",
        "    if not hasattr(sequences, '__len__'):\n",
        "        raise ValueError('`sequences` must be iterable.')\n",
        "    lengths = []\n",
        "    for x in sequences:\n",
        "        if not hasattr(x, '__len__'):\n",
        "            raise ValueError('`sequences` must be a list of iterables. '\n",
        "                             'Found non-iterable: ' + str(x))\n",
        "        lengths.append(len(x))\n",
        "\n",
        "    num_samples = len(sequences)\n",
        "    if maxlen is None:\n",
        "        maxlen = np.max(lengths)\n",
        "\n",
        "    # take the sample shape from the first non empty sequence\n",
        "    # checking for consistency in the main loop below.\n",
        "    sample_shape = tuple()\n",
        "    for s in sequences:\n",
        "        if len(s) > 0:\n",
        "            sample_shape = np.asarray(s).shape[1:]\n",
        "            break\n",
        "\n",
        "    is_dtype_str = np.issubdtype(dtype, np.str_) or np.issubdtype(dtype, np.unicode_)\n",
        "    if isinstance(value, six.string_types) and dtype != object and not is_dtype_str:\n",
        "        raise ValueError(\"`dtype` {} is not compatible with `value`'s type: {}\\n\"\n",
        "                         \"You should set `dtype=object` for variable length strings.\"\n",
        "                         .format(dtype, type(value)))\n",
        "\n",
        "    x = np.full((num_samples, maxlen) + sample_shape, value, dtype=dtype)\n",
        "    for idx, s in enumerate(sequences):\n",
        "        if not len(s):\n",
        "            continue  # empty list/array was found\n",
        "        if truncating == 'pre':\n",
        "            trunc = s[-maxlen:]\n",
        "        elif truncating == 'post':\n",
        "            trunc = s[:maxlen]\n",
        "        else:\n",
        "            raise ValueError('Truncating type \"%s\" '\n",
        "                             'not understood' % truncating)\n",
        "\n",
        "        # check `trunc` has expected shape\n",
        "        trunc = np.asarray(trunc, dtype=dtype)\n",
        "        if trunc.shape[1:] != sample_shape:\n",
        "            raise ValueError('Shape of sample %s of sequence at position %s '\n",
        "                             'is different from expected shape %s' %\n",
        "                             (trunc.shape[1:], idx, sample_shape))\n",
        "\n",
        "        if padding == 'post':\n",
        "            x[idx, :len(trunc)] = trunc\n",
        "        elif padding == 'pre':\n",
        "            x[idx, -len(trunc):] = trunc\n",
        "        else:\n",
        "            raise ValueError('Padding type \"%s\" not understood' % padding)\n",
        "    return x\n",
        "\n",
        "\n",
        "# https://github.com/keras-team/keras/blob/master/keras/utils/np_utils.py#L9-L37\n",
        "def to_categorical(y, num_classes=None, dtype='float32'):\n",
        "    \"\"\"Converts a class vector (integers) to binary class matrix.\n",
        "    E.g. for use with categorical_crossentropy.\n",
        "    # Arguments\n",
        "        y: class vector to be converted into a matrix\n",
        "            (integers from 0 to num_classes).\n",
        "        num_classes: total number of classes.\n",
        "        dtype: The data type expected by the input, as a string\n",
        "            (`float32`, `float64`, `int32`...)\n",
        "    # Returns\n",
        "        A binary matrix representation of the input. The classes axis\n",
        "        is placed last.\n",
        "    # Example\n",
        "    ```python\n",
        "    # Consider an array of 5 labels out of a set of 3 classes {0, 1, 2}:\n",
        "    > labels\n",
        "    array([0, 2, 1, 2, 0])\n",
        "    # `to_categorical` converts this into a matrix with as many\n",
        "    # columns as there are classes. The number of rows\n",
        "    # stays the same.\n",
        "    > to_categorical(labels)\n",
        "    array([[ 1.,  0.,  0.],\n",
        "           [ 0.,  0.,  1.],\n",
        "           [ 0.,  1.,  0.],\n",
        "           [ 0.,  0.,  1.],\n",
        "           [ 1.,  0.,  0.]], dtype=float32)\n",
        "    ```\n",
        "    \"\"\"\n",
        "\n",
        "    y = np.array(y, dtype='int')\n",
        "    input_shape = y.shape\n",
        "    if input_shape and input_shape[-1] == 1 and len(input_shape) > 1:\n",
        "        input_shape = tuple(input_shape[:-1])\n",
        "    y = y.ravel()\n",
        "    if not num_classes:\n",
        "        num_classes = np.max(y) + 1\n",
        "    n = y.shape[0]\n",
        "    categorical = np.zeros((n, num_classes), dtype=dtype)\n",
        "    categorical[np.arange(n), y] = 1\n",
        "    output_shape = input_shape + (num_classes,)\n",
        "    categorical = np.reshape(categorical, output_shape)\n",
        "    return categorical\n",
        "  \n",
        "def configure_plotly_browser_state():\n",
        "    import IPython\n",
        "    display(IPython.core.display.HTML('''\n",
        "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
        "          <script>\n",
        "            requirejs.config({\n",
        "              paths: {\n",
        "                base: '/static/base',\n",
        "                plotly: 'https://cdn.plot.ly/plotly-latest.min.js?noext',\n",
        "              },\n",
        "            });\n",
        "          </script>\n",
        "          '''))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OpJjtoD5QuSO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Source"
      ]
    },
    {
      "metadata": {
        "id": "S_PcJCNYIe6x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_alphabet():\n",
        "    \"\"\"\n",
        "    All the characters that might be seen in a sequence\n",
        "    \"\"\"\n",
        "    return \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
        "\n",
        "\n",
        "def get_sequence(name=\"alphabet\"):\n",
        "\n",
        "    sequences = dict(\n",
        "        alphabet=\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\",\n",
        "        # Repeated letter H\n",
        "        repeatone=\"ABCDEFGHIJKLMNOPHRSTUVWXYZ\",\n",
        "        # Repeat five letters - Seq len 30\n",
        "        # [C, D, F, J, T]\n",
        "        repeatfive=\"ABCDETFGCHIJKLDMNOPFQRSTUJVWXYZ\",\n",
        "        # A sequence of 30 randomly selected characters\n",
        "        random30=\"VNMVWKTFFLOZKUNSDYGOEFOBTZTTVU\",\n",
        "    )\n",
        "    return sequences[name]\n",
        "\n",
        "\n",
        "def get_char_to_int(alphabet):\n",
        "    char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
        "    return char_to_int\n",
        "\n",
        "\n",
        "def get_int_to_char(alphabet):\n",
        "    int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
        "    return int_to_char\n",
        "\n",
        "\n",
        "def get_data(alphabet, sequence):\n",
        "\n",
        "    # Get mapping of characters to integers (0-25) and the reverse\n",
        "    char_to_int = get_char_to_int(alphabet)\n",
        "\n",
        "    # Prepare the dataset of input to output pairs encoded as integers\n",
        "    dataX = []\n",
        "    y = []\n",
        "    for i in range(0, len(sequence)):\n",
        "        # 0 - 25\n",
        "        char_in = sequence[i]\n",
        "        next_ind = (i + 1) % len(sequence)\n",
        "        char_out = sequence[next_ind]\n",
        "        dataX.append([char_to_int[char_in]])\n",
        "        y.append(char_to_int[char_out])\n",
        "        print(char_in, '->', char_out)\n",
        "\n",
        "    # Convert list of lists to array and pad sequences if needed\n",
        "    X = pad_sequences(dataX, maxlen=1, dtype='float32')\n",
        "\n",
        "    # Reshape X to be [samples, time steps, features]\n",
        "    X = np.reshape(dataX, (X.shape[0], 1, 1))\n",
        "\n",
        "    # One hot encode input\n",
        "    X = to_categorical(X, len(alphabet))\n",
        "    \n",
        "    # Convert our training data into a tensor\n",
        "    X = torch.Tensor(X)\n",
        "\n",
        "    return X, y\n",
        "\n",
        "def letter_to_one_hot_tensor(letter):\n",
        "    alphabet = get_alphabet()\n",
        "    char_to_int = get_char_to_int(alphabet)\n",
        "\n",
        "    letter_as_int = char_to_int[letter]\n",
        "    l_as_array = [[letter_as_int]]\n",
        "    l_as_one_hot = to_categorical(l_as_array, len(alphabet))\n",
        "    one_hot_tensor = torch.Tensor(l_as_one_hot)\n",
        "    return one_hot_tensor\n",
        "\n",
        "\n",
        "def one_hot_tensor_to_letter(one_hot_tensor):\n",
        "    one_hot_tensor = one_hot_tensor.detach()\n",
        "    alphabet = get_alphabet()\n",
        "    int_to_char = get_int_to_char(alphabet)\n",
        "    in_int = int(np.argmax(one_hot_tensor[0]))\n",
        "    letter = int_to_char[in_int]\n",
        "    return letter\n",
        "\n",
        "\n",
        "def run_forward_output(model, start, steps):\n",
        "    x = letter_to_one_hot_tensor(start)\n",
        "    for i in range(steps):\n",
        "        in_char = one_hot_tensor_to_letter(x)\n",
        "        output, state, pred = model(x)\n",
        "        out_char = one_hot_tensor_to_letter(output)\n",
        "        print(in_char, \"->\", out_char)\n",
        "        # Feed the prediction back in to the next timestep\n",
        "        x = output\n",
        "\n",
        "\n",
        "def run_forward_pred(model, sequence, start=0, steps=25, one_step=False):\n",
        "    \"\"\"\n",
        "    Run the model forward for <steps> steps starting at <start>\n",
        "    index in the sequence, feeding the output of the model back\n",
        "    in as the input in the next step.\n",
        "    \"\"\"\n",
        "    start_letter = sequence[start]\n",
        "    x = letter_to_one_hot_tensor(start_letter)\n",
        "    steps_correct = 0\n",
        "    broken = False\n",
        "    generated = []\n",
        "    # Make sure model is in a clean starting state\n",
        "    model.reset_prev_state()\n",
        "    for i in range(steps):\n",
        "        # If the model was trained without state evaluate in the same way\n",
        "        if one_step:\n",
        "          model.reset_prev_state()\n",
        "        in_char = one_hot_tensor_to_letter(x)\n",
        "        output = model(x)\n",
        "        out_char = one_hot_tensor_to_letter(output)\n",
        "        generated.append(out_char)\n",
        "        next_letter = sequence[(start + i + 1) % len(sequence)]\n",
        "\n",
        "        # Feed the prediction back in to the next timestep\n",
        "        x = output\n",
        "        if out_char != next_letter:\n",
        "            broken = True\n",
        "        if not broken:\n",
        "            steps_correct += 1\n",
        "\n",
        "    target_sequence = sequence[1:]\n",
        "    print(\"Target Sequence:\", target_sequence)\n",
        "    generated = \"\".join(generated)\n",
        "    print(\"Produced Sequence:\", generated)\n",
        "    matcher = SequenceMatcher(None, target_sequence, generated)\n",
        "    match = matcher.find_longest_match(0, len(target_sequence), 0, len(generated))\n",
        "    longest_substring = target_sequence[match.a: match.a + match.size]\n",
        "    longest_len = len(longest_substring)\n",
        "    print(\"Longest Substring:\", longest_substring)\n",
        "\n",
        "    print(\"Initial steps without breaking: {}\".format(steps_correct))\n",
        "    print(\"Longest substring produced: {}\".format(longest_len))\n",
        "    return steps_correct\n",
        "\n",
        "\n",
        "def evaluate_model(model, sequence, one_step):\n",
        "\n",
        "    steps_correct = run_forward_pred(\n",
        "        model,\n",
        "        sequence,\n",
        "        start=0, \n",
        "        steps=len(sequence) - 1,\n",
        "        one_step=one_step\n",
        "    )\n",
        "    return steps_correct == len(sequence) - 1\n",
        "    \n",
        "\n",
        "class LSTMMod(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.lin = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.Softmax(dim=2)\n",
        "\n",
        "        # Init hidden state and context to zeros\n",
        "        self.reset_prev_state()\n",
        "\n",
        "    def _get_prev_state_size(self):\n",
        "        return self.hidden_size\n",
        "\n",
        "    def _store_prev(self, hidden, context):\n",
        "        self.prev_state = hidden\n",
        "        self.prev_context = context\n",
        "\n",
        "    def reset_prev_state(self):\n",
        "        self.prev_state = torch.zeros(1, 1, self._get_prev_state_size())\n",
        "        self.prev_state.requires_grad = True\n",
        "\n",
        "        self.prev_context = torch.zeros(1, 1, self._get_prev_state_size())\n",
        "        self.prev_context.requires_grad = True\n",
        "\n",
        "    def forward(self, inp):\n",
        "        inp = inp.unsqueeze(0)  # Add extra batch dimension\n",
        "        hidden, state = self.lstm(inp, (self.prev_state, self.prev_context))\n",
        "        h_n, c_n = state\n",
        "        lin_out = self.lin(hidden)\n",
        "        output = self.softmax(lin_out)\n",
        "\n",
        "        output = output.squeeze(dim=0)  # Remove extra dimension\n",
        "\n",
        "        self._store_prev(hidden, c_n)\n",
        "        return output\n",
        "\n",
        "\n",
        "def train(model, loss_module, opt, alphabet, sequence, max_epochs, one_step=False):\n",
        "    X, y = get_data(alphabet, sequence)\n",
        "\n",
        "    ff_losses = []\n",
        "\n",
        "    for i in range(max_epochs):\n",
        "        if i % 10 == 0:\n",
        "            print(\" ========== EPOCH: {} ============\".format(i))\n",
        "            \n",
        "            if evaluate_model(model, sequence, one_step):\n",
        "              print(\"Training SUCCESSFUL.\")\n",
        "              break\n",
        "        # Reset state at the beginning of each epoch\n",
        "        model.reset_prev_state()\n",
        "        # Feed the network the sequence character by character\n",
        "        for j, inp in enumerate(X):\n",
        "            # If set reset state after each character and truncate\n",
        "            # backprop to a single step\n",
        "            if one_step:\n",
        "                model.reset_prev_state()\n",
        "            next_ind = (j + 1) % len(sequence)\n",
        "            next_char = sequence[next_ind]\n",
        "            label = letter_to_one_hot_tensor(next_char)\n",
        "            # Labels must be Long tensors\n",
        "            # This exact format is required\n",
        "            label = torch.argmax(label, dim=1)\n",
        "\n",
        "            output = model(inp)\n",
        "\n",
        "            # Compute the feed forward loss\n",
        "            loss = loss_module(output, label)\n",
        "            ff_losses.append(loss.detach().item())\n",
        "\n",
        "            # Zero gradients\n",
        "            opt.zero_grad()\n",
        "\n",
        "            # Calculate the gradients of the losses\n",
        "            loss.backward(retain_graph=True)\n",
        "\n",
        "            # Take an optimization step\n",
        "            opt.step()\n",
        "\n",
        "    return ff_losses\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lFxIl6IMRKrk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Main"
      ]
    },
    {
      "metadata": {
        "id": "1xWIYxmdekqY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def run_experiment(sequence, one_step, units=32, max_epochs=1000):\n",
        "    alphabet = get_alphabet()\n",
        "\n",
        "    model = LSTMMod(\n",
        "        input_size=len(alphabet),\n",
        "        hidden_size=units,\n",
        "        output_size=len(alphabet)\n",
        "    )\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    loss_module = nn.CrossEntropyLoss()\n",
        "    max_epochs = 1000\n",
        "\n",
        "    ff_losses = train(\n",
        "        model,\n",
        "        loss_module,\n",
        "        optimizer,\n",
        "        alphabet,\n",
        "        sequence,\n",
        "        max_epochs,\n",
        "        one_step\n",
        "    )\n",
        "\n",
        "    print(\"Double check the evaluation ...\")\n",
        "    evaluate_model(model, sequence, one_step)\n",
        "\n",
        "    # Plot losses from training run\n",
        "    configure_plotly_browser_state()\n",
        "    init_notebook_mode(connected=False)\n",
        "\n",
        "    ff_losses = np.array(ff_losses)\n",
        "    ff_loss_trace = dict(y=ff_losses, name=\"Feed Forward\")\n",
        "    data = [\n",
        "        ff_loss_trace,\n",
        "    ]\n",
        "\n",
        "    plot(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ouHL7-ZxXumG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Case 1 (Working)\n",
        "\n",
        "Target sequence, the alphabet itself, \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
        "\n",
        "Here we train an LSTM with 32 units as if it were a feed-forward network. \n",
        "\n",
        "We reset state after each character fed to the network, and thus backprop is only 1 step. (Controlled by the variable `one_step`)\n",
        "\n",
        "This converges in around 50-100 epochs."
      ]
    },
    {
      "metadata": {
        "id": "oBy_2ChMJlkV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sequence = get_sequence(\"alphabet\")\n",
        "one_step = True\n",
        "\n",
        "run_experiment(sequence, one_step)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0GZOOVHTY1uz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Case 2 (Working)\n",
        "\n",
        "Target sequence, the alphabet itself, \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
        "\n",
        "Here we train an LSTM with 32 units as normal for an RNN.\n",
        "\n",
        "We **DO NOT** reset state after each character fed to the network, we **do** reset after each epoch, and thus backprop is at most 25 steps.\n",
        "\n",
        "This converges in around 50-100 epochs.\n",
        "\n",
        "Loss reduction over time is less smooth."
      ]
    },
    {
      "metadata": {
        "id": "dEWRSNsDPENq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sequence = get_sequence(\"alphabet\")\n",
        "one_step = False\n",
        "\n",
        "run_experiment(sequence, one_step)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kgM_Ylq-ZxIn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Case 3 (Failing - But Expected)\n",
        "\n",
        "Target sequence has a **single repeated character**, the letter \"H\" appears twice, and there is no \"Q\"\n",
        "\n",
        "\"ABCDEFGHIJKLMNOPHRSTUVWXYZ\"\n",
        "\n",
        "We reset state after each character fed to the network.\n",
        "\n",
        "There is no way for a network trained in this manner to correctly predict BOTH the transitions:\n",
        "\n",
        "G -> H\n",
        "\n",
        "P -> H\n",
        "\n",
        "This reaches `max_epochs` without converging"
      ]
    },
    {
      "metadata": {
        "id": "VTw3HQDbZMTi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sequence = get_sequence(\"repeatone\")\n",
        "one_step = True\n",
        "\n",
        "run_experiment(sequence, one_step)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RxNeZqDtcTrD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Case 4 (Working - Slower)\n",
        "\n",
        "Target sequence has a **single repeated character**, the letter \"H\" appears twice, and there is no \"Q\"\n",
        "\n",
        "\"ABCDEFGHIJKLMNOPHRSTUVWXYZ\"\n",
        "\n",
        "We **DO NOT** reset state after each character fed to the network.\n",
        "\n",
        "This converges after 100 to 200 epochs\n"
      ]
    },
    {
      "metadata": {
        "id": "3jtL3dujakD5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sequence = get_sequence(\"repeatone\")\n",
        "one_step = False\n",
        "\n",
        "run_experiment(sequence, one_step)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MR8sqWtBeKp_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Case 5 (Working - Slow)\n",
        "\n",
        "Target sequence has a **five repeated characters**\n",
        "\n",
        "\"ABCDETFGCHIJKLDMNOPFQRSTUJVWXYZ\"\n",
        "\n",
        "We **DO NOT** reset state after each character fed to the network.\n",
        "\n",
        "This often converges after 200-300 epochs"
      ]
    },
    {
      "metadata": {
        "id": "VlTC7LSSc6qZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sequence = get_sequence(\"repeatfive\")\n",
        "one_step = False\n",
        "\n",
        "run_experiment(sequence, one_step)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yiklVLMLhJz3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Case 6 (Failing)\n",
        "\n",
        "Target sequence is 30 characters that were selected randomly from the alphabet (with replacement)\n",
        "\n",
        "\"VNMVWKTFFLOZKUNSDYGOEFOBTZTTVU\"\n",
        "\n",
        "We **DO NOT** reset state after each character fed to the network.\n",
        "\n",
        "This fails to converge after 1000 epochs.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "uQejbOCTgeme",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sequence = get_sequence(\"random30\")\n",
        "one_step = False\n",
        "\n",
        "run_experiment(sequence, one_step)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "JFgwuawfi91p"
      },
      "cell_type": "markdown",
      "source": [
        "## Case 7 (Working - Slow)\n",
        "\n",
        "Here we significantly increase the size of the network to have **128 hidden units**.\n",
        "\n",
        "Target sequence is 30 characters that were selected randomly from the alphabet (with replacement)\n",
        "\n",
        "\"VNMVWKTFFLOZKUNSDYGOEFOBTZTTVU\"\n",
        "\n",
        "We **DO NOT** reset state after each character fed to the network.\n",
        "\n",
        "This often converges after 400-600 epochs\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "bkQxtIEpi910",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sequence = get_sequence(\"random30\")\n",
        "one_step = False\n",
        "\n",
        "run_experiment(sequence, one_step, units=128, max_epochs=2000)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Generate your own Simpsons TV script using Deep Learning.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0MnXQhQZUth",
        "colab_type": "text"
      },
      "source": [
        "# How to generate your own \"The Simpsons\" TV script using Deep Learning\n",
        "**Have you ever dreamed of creating your own episode of The Simpsons? I did.**\n",
        "\n",
        "**This is the notebook for the story published at (https://towardsdatascience.com/how-to-generate-your-own-the-simpsons-tv-script-using-deep-learning-980337173796)** \n",
        "\n",
        "That is what i thought when i saw the Simpsons dataset at Kaggle. It is the perfect dataset for a small \"just for fun\" project on Natural Language Generation (NLG).\n",
        "\n",
        "### What is Natural Language Generation (NLG)?\n",
        "\n",
        "**Natural-language generation (NLG) is the aspect of language technology that focuses on generating natural language from structured data or structured representations such as a knowledge base or a logical form.\n",
        "(https://en.wikipedia.org/wiki/Natural-language_generation)**\n",
        "\n",
        "In this case we will see how to train a model that will be capable of creating new \"Simpsons-Style\" conversations. As input for the training we will use the  file simpsons_script_lines.csv from the Simpsons dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zG52g6lZUtm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import pickle\n",
        "import re\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "from IPython.display import display\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.contrib import seq2seq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKeSXzl2ZUts",
        "colab_type": "text"
      },
      "source": [
        "### Downloading and preparing the data\n",
        "First you need to download the data file. You can do this on the Kaggle website of \"The Simpsons by the Data\". Download the file [simpsons_script_lines.csv](https://medium.com/r/?url=https%3A%2F%2Fwww.kaggle.com%2Fwcukierski%2Fthe-simpsons-by-the-data%23simpsons_script_lines.csv), save it to a folder \"data\" and unzip it. It should be ~34MB after unzipping."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "_bCy-ofHZUts",
        "colab_type": "code",
        "colab": {},
        "outputId": "502625b0-cff2-4144-e232-c22306a71318"
      },
      "source": [
        "data_dir = './data/simpsons_script_lines.csv'\n",
        "input_file = os.path.join(data_dir)\n",
        "\n",
        "clean_text = ''\n",
        "\n",
        "with open(input_file, \"r\", encoding=\"utf8\") as f:\n",
        "    for line in f:\n",
        "        text = re.search('[0-9]*,[0-9]*,[0-9]*,(.+?),[0-9]*,', line)\n",
        "        if text:\n",
        "            text = text.group(1).replace('\"', '')\n",
        "            text_parts = text.split(':')\n",
        "            text_parts[0] = text_parts[0].replace(' ', '_')\n",
        "            text = ':'.join(text_parts)\n",
        "            clean_text += text + '\\n'\n",
        "\n",
        "print('\\n'.join(clean_text.split('\\n')[:10]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Miss_Hoover: No, actually, it was a little of both. Sometimes when a disease is in all the magazines and all the news shows, it's only natural that you think you have it.\n",
            "Lisa_Simpson: (NEAR TEARS) Where's Mr. Bergstrom?\n",
            "Miss_Hoover: I don't know. Although I'd sure like to talk to him. He didn't touch my lesson plan. What did he teach you?\n",
            "Lisa_Simpson: That life is worth living.\n",
            "Edna_Krabappel-Flanders: The polls will be open from now until the end of recess. Now, (SOUR) just in case any of you have decided to put any thought into this, we'll have our final statements. Martin?\n",
            "Martin_Prince: (HOARSE WHISPER) I don't think there's anything left to say.\n",
            "Edna_Krabappel-Flanders: Bart?\n",
            "Bart_Simpson: Victory party under the slide!\n",
            "(Apartment_Building: Ext. apartment building - day)\n",
            "Lisa_Simpson: (CALLING) Mr. Bergstrom! Mr. Bergstrom!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvihXC8CZUt0",
        "colab_type": "text"
      },
      "source": [
        "### Data preprocessing\n",
        "\n",
        "Before we can use this as input for training of our model we first need to do some additional preprocessing.\n",
        "\n",
        "We'll be splitting the script into a word array using spaces as delimiters. However, punctuations like periods and exclamation marks make it hard for the neural network to distinguish between the word \"bye\" and \"bye!\".\n",
        "\n",
        "To solve this we create a dictionary that we will use to token the symbols and add the delimiter (space) around it. This separates the symbols from the words, making it easier for the neural network to predict on the next word.\n",
        "\n",
        "In the next step we will use this dictionary to replace the symbols, build the vocabulary and lookup table for the words in the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhU9FcSiZUt1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenized_punctuation = {\n",
        "    '.' : '||Period||',\n",
        "    ',' : '||Comma||',\n",
        "    '\"' : '||Quotation_Mark||',\n",
        "    ';' : '||Semicolon||',\n",
        "    '!' : '||Exclamation_Mark||',\n",
        "    '?' : '||Question_Mark||',\n",
        "    '(' : '||Left_Parentheses||',\n",
        "    ')' : '||Right_Parentheses||',\n",
        "    '--' : '||Dash||',\n",
        "    '\\n' : '||Return||'\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYYukqQtZUt4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for key, token in tokenized_punctuation.items():\n",
        "    clean_text = clean_text.replace(key, ' {} '.format(token))\n",
        "\n",
        "clean_text = clean_text.lower()\n",
        "clean_text = clean_text.split()\n",
        "\n",
        "word_counts = Counter(clean_text)\n",
        "sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "int_to_vocab = {ii: word for ii, word in enumerate(sorted_vocab)}\n",
        "vocab_to_int = {word: ii for ii, word in int_to_vocab.items()} \n",
        "\n",
        "int_text = [vocab_to_int[word] for word in clean_text]\n",
        "pickle.dump((int_text, vocab_to_int, int_to_vocab, tokenized_punctuation), open('preprocess.p', 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JCeITi4ZUt_",
        "colab_type": "text"
      },
      "source": [
        "### Build the Neural Network\n",
        "Now that we have prepared the data it is time to create the neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ur_P62eOZUuA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_inputs():\n",
        "    input_placeholder = tf.placeholder(tf.int32, [None, None], name = 'input')\n",
        "    targets_placeholder = tf.placeholder(tf.int32, [None, None])\n",
        "    learning_rate_placeholder = tf.placeholder(tf.float32)\n",
        "    \n",
        "    return input_placeholder, targets_placeholder, learning_rate_placeholder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9dlUXqcZUuD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_init_cell(batch_size, rnn_size):\n",
        "    lstm = tf.contrib.rnn.GRUCell(rnn_size)\n",
        "    cell = tf.contrib.rnn.MultiRNNCell([lstm])\n",
        "    initial_state = tf.identity(cell.zero_state(batch_size, tf.float32), name='initial_state')\n",
        "    return cell, initial_state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdDSSn6UZUuI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_embed(input_data, vocab_size, embed_dim):\n",
        "    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n",
        "    embed = tf.nn.embedding_lookup(embedding, input_data)    \n",
        "    return embed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHzYV5w8ZUuL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_rnn(cell, inputs):\n",
        "    outputs, state = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32)\n",
        "    final_state = tf.identity(state, name=\"final_state\")\n",
        "    return outputs, final_state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnfR4BvgZUuP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_nn(cell, rnn_size, input_data, vocab_size, embed_dim):\n",
        "    embeddings = get_embed(input_data, vocab_size, embed_dim)\n",
        "    inputs, final_state = build_rnn(cell, embeddings)\n",
        "    logits = tf.contrib.layers.fully_connected(inputs=inputs, num_outputs=vocab_size, activation_fn=None)\n",
        "    return logits, final_state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbLWXiR_ZUuS",
        "colab_type": "text"
      },
      "source": [
        "### Training the Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cIfTXTiZUuV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batches(int_text, batch_size, seq_length):\n",
        "    n_batches = len(int_text) // (batch_size * seq_length)\n",
        "    words = np.asarray(int_text[:n_batches*(batch_size * seq_length)])\n",
        "    \n",
        "    batches = np.zeros(shape=(n_batches, 2, batch_size, seq_length))\n",
        "\n",
        "    input_sequences = words.reshape(-1, seq_length)\n",
        "    target_sequences = np.roll(words, -1)\n",
        "    target_sequences = target_sequences.reshape(-1, seq_length)\n",
        "    \n",
        "    for idx in range(0, input_sequences.shape[0]):\n",
        "        input_idx = idx % n_batches\n",
        "        target_idx = idx // n_batches\n",
        "        batches[input_idx,0,target_idx,:] = input_sequences[idx,:]\n",
        "        batches[input_idx,1,target_idx,:] = target_sequences[idx,:]        \n",
        "    return batches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-4wGrROZUuY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Number of Epochs\n",
        "num_epochs = 50\n",
        "# Batch Size\n",
        "batch_size = 32\n",
        "# RNN Size\n",
        "rnn_size = 512\n",
        "# Embedding Dimension Size\n",
        "embed_dim = 256\n",
        "# Sequence Length\n",
        "seq_length = 16\n",
        "# Learning Rate\n",
        "learning_rate = 0.001\n",
        "# Show stats for every n number of batches\n",
        "show_every_n_batches = 200\n",
        "\n",
        "save_dir = './save'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wK0t4RXHZUub",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_graph = tf.Graph()\n",
        "with train_graph.as_default():\n",
        "    vocab_size = len(int_to_vocab)\n",
        "    input_text, targets, lr = get_inputs()\n",
        "    input_data_shape = tf.shape(input_text)\n",
        "    cell, initial_state = get_init_cell(input_data_shape[0], rnn_size)\n",
        "    logits, final_state = build_nn(cell, rnn_size, input_text, vocab_size, embed_dim)\n",
        "\n",
        "    # Probabilities for generating words\n",
        "    probs = tf.nn.softmax(logits, name='probs')\n",
        "\n",
        "    # Loss function\n",
        "    cost = seq2seq.sequence_loss(\n",
        "        logits,\n",
        "        targets,\n",
        "        tf.ones([input_data_shape[0], input_data_shape[1]]))\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = tf.train.AdamOptimizer(lr)\n",
        "\n",
        "    # Gradient Clipping\n",
        "    gradients = optimizer.compute_gradients(cost)\n",
        "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
        "    train_op = optimizer.apply_gradients(capped_gradients)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isagyTiiZUue",
        "colab_type": "code",
        "colab": {},
        "outputId": "a4df4545-5b8d-4d9c-af52-e14f95a6ec52"
      },
      "source": [
        "batches = get_batches(int_text, batch_size, seq_length)\n",
        "\n",
        "with tf.Session(graph=train_graph) as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    for epoch_i in range(num_epochs):\n",
        "        state = sess.run(initial_state, {input_text: batches[0][0]})\n",
        "\n",
        "        for batch_i, (x, y) in enumerate(batches):\n",
        "            feed = {\n",
        "                input_text: x,\n",
        "                targets: y,\n",
        "                initial_state: state,\n",
        "                lr: learning_rate}\n",
        "            train_loss, state, _ = sess.run([cost, final_state, train_op], feed)\n",
        "\n",
        "            # Show every <show_every_n_batches> batches\n",
        "            if (epoch_i * len(batches) + batch_i) % show_every_n_batches == 0:\n",
        "                print('Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}'.format(\n",
        "                    epoch_i,\n",
        "                    batch_i,\n",
        "                    len(batches),\n",
        "                    train_loss))\n",
        "\n",
        "    # Save Model\n",
        "    saver = tf.train.Saver()\n",
        "    saver.save(sess, save_dir)\n",
        "    print('Model Trained and Saved')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch   0 Batch    0/4686   train_loss = 11.039\n",
            "Epoch   0 Batch  200/4686   train_loss = 6.892\n",
            "Epoch   0 Batch  400/4686   train_loss = 6.000\n",
            "Epoch   0 Batch  600/4686   train_loss = 5.782\n",
            "Epoch   0 Batch  800/4686   train_loss = 5.047\n",
            "Epoch   0 Batch 1000/4686   train_loss = 4.870\n",
            "Epoch   0 Batch 1200/4686   train_loss = 4.674\n",
            "Epoch   0 Batch 1400/4686   train_loss = 4.906\n",
            "Epoch   0 Batch 1600/4686   train_loss = 4.652\n",
            "Epoch   0 Batch 1800/4686   train_loss = 4.736\n",
            "Epoch   0 Batch 2000/4686   train_loss = 4.699\n",
            "Epoch   0 Batch 2200/4686   train_loss = 4.515\n",
            "Epoch   0 Batch 2400/4686   train_loss = 4.317\n",
            "Epoch   0 Batch 2600/4686   train_loss = 4.358\n",
            "Epoch   0 Batch 2800/4686   train_loss = 4.982\n",
            "Epoch   0 Batch 3000/4686   train_loss = 4.443\n",
            "Epoch   0 Batch 3200/4686   train_loss = 4.373\n",
            "Epoch   0 Batch 3400/4686   train_loss = 4.505\n",
            "Epoch   0 Batch 3600/4686   train_loss = 4.286\n",
            "Epoch   0 Batch 3800/4686   train_loss = 4.423\n",
            "Epoch   0 Batch 4000/4686   train_loss = 4.398\n",
            "Epoch   0 Batch 4200/4686   train_loss = 4.234\n",
            "Epoch   0 Batch 4400/4686   train_loss = 4.560\n",
            "Epoch   0 Batch 4600/4686   train_loss = 4.401\n",
            "Epoch   1 Batch  114/4686   train_loss = 4.471\n",
            "Epoch   1 Batch  314/4686   train_loss = 4.427\n",
            "Epoch   1 Batch  514/4686   train_loss = 4.477\n",
            "Epoch   1 Batch  714/4686   train_loss = 4.087\n",
            "Epoch   1 Batch  914/4686   train_loss = 4.141\n",
            "Epoch   1 Batch 1114/4686   train_loss = 4.069\n",
            "Epoch   1 Batch 1314/4686   train_loss = 4.382\n",
            "Epoch   1 Batch 1514/4686   train_loss = 4.043\n",
            "Epoch   1 Batch 1714/4686   train_loss = 3.921\n",
            "Epoch   1 Batch 1914/4686   train_loss = 4.184\n",
            "Epoch   1 Batch 2114/4686   train_loss = 3.818\n",
            "Epoch   1 Batch 2314/4686   train_loss = 4.087\n",
            "Epoch   1 Batch 2514/4686   train_loss = 3.726\n",
            "Epoch   1 Batch 2714/4686   train_loss = 3.799\n",
            "Epoch   1 Batch 2914/4686   train_loss = 4.126\n",
            "Epoch   1 Batch 3114/4686   train_loss = 3.739\n",
            "Epoch   1 Batch 3314/4686   train_loss = 3.836\n",
            "Epoch   1 Batch 3514/4686   train_loss = 4.147\n",
            "Epoch   1 Batch 3714/4686   train_loss = 3.703\n",
            "Epoch   1 Batch 3914/4686   train_loss = 3.839\n",
            "Epoch   1 Batch 4114/4686   train_loss = 3.564\n",
            "Epoch   1 Batch 4314/4686   train_loss = 3.729\n",
            "Epoch   1 Batch 4514/4686   train_loss = 3.408\n",
            "Epoch   2 Batch   28/4686   train_loss = 3.870\n",
            "Epoch   2 Batch  228/4686   train_loss = 3.876\n",
            "Epoch   2 Batch  428/4686   train_loss = 3.637\n",
            "Epoch   2 Batch  628/4686   train_loss = 3.496\n",
            "Epoch   2 Batch  828/4686   train_loss = 3.738\n",
            "Epoch   2 Batch 1028/4686   train_loss = 3.555\n",
            "Epoch   2 Batch 1228/4686   train_loss = 3.638\n",
            "Epoch   2 Batch 1428/4686   train_loss = 3.762\n",
            "Epoch   2 Batch 1628/4686   train_loss = 3.664\n",
            "Epoch   2 Batch 1828/4686   train_loss = 3.616\n",
            "Epoch   2 Batch 2028/4686   train_loss = 3.559\n",
            "Epoch   2 Batch 2228/4686   train_loss = 3.531\n",
            "Epoch   2 Batch 2428/4686   train_loss = 3.558\n",
            "Epoch   2 Batch 2628/4686   train_loss = 3.460\n",
            "Epoch   2 Batch 2828/4686   train_loss = 3.467\n",
            "Epoch   2 Batch 3028/4686   train_loss = 3.490\n",
            "Epoch   2 Batch 3228/4686   train_loss = 3.525\n",
            "Epoch   2 Batch 3428/4686   train_loss = 3.263\n",
            "Epoch   2 Batch 3628/4686   train_loss = 3.679\n",
            "Epoch   2 Batch 3828/4686   train_loss = 3.485\n",
            "Epoch   2 Batch 4028/4686   train_loss = 3.471\n",
            "Epoch   2 Batch 4228/4686   train_loss = 3.158\n",
            "Epoch   2 Batch 4428/4686   train_loss = 3.351\n",
            "Epoch   2 Batch 4628/4686   train_loss = 3.198\n",
            "Epoch   3 Batch  142/4686   train_loss = 3.364\n",
            "Epoch   3 Batch  342/4686   train_loss = 3.329\n",
            "Epoch   3 Batch  542/4686   train_loss = 3.518\n",
            "Epoch   3 Batch  742/4686   train_loss = 3.424\n",
            "Epoch   3 Batch  942/4686   train_loss = 3.275\n",
            "Epoch   3 Batch 1142/4686   train_loss = 3.534\n",
            "Epoch   3 Batch 1342/4686   train_loss = 3.073\n",
            "Epoch   3 Batch 1542/4686   train_loss = 3.181\n",
            "Epoch   3 Batch 1742/4686   train_loss = 3.531\n",
            "Epoch   3 Batch 1942/4686   train_loss = 3.055\n",
            "Epoch   3 Batch 2142/4686   train_loss = 2.989\n",
            "Epoch   3 Batch 2342/4686   train_loss = 3.149\n",
            "Epoch   3 Batch 2542/4686   train_loss = 3.153\n",
            "Epoch   3 Batch 2742/4686   train_loss = 3.220\n",
            "Epoch   3 Batch 2942/4686   train_loss = 3.028\n",
            "Epoch   3 Batch 3142/4686   train_loss = 2.998\n",
            "Epoch   3 Batch 3342/4686   train_loss = 3.229\n",
            "Epoch   3 Batch 3542/4686   train_loss = 3.041\n",
            "Epoch   3 Batch 3742/4686   train_loss = 3.166\n",
            "Epoch   3 Batch 3942/4686   train_loss = 2.958\n",
            "Epoch   3 Batch 4142/4686   train_loss = 3.025\n",
            "Epoch   3 Batch 4342/4686   train_loss = 3.210\n",
            "Epoch   3 Batch 4542/4686   train_loss = 3.237\n",
            "Epoch   4 Batch   56/4686   train_loss = 3.167\n",
            "Epoch   4 Batch  256/4686   train_loss = 3.295\n",
            "Epoch   4 Batch  456/4686   train_loss = 3.200\n",
            "Epoch   4 Batch  656/4686   train_loss = 3.126\n",
            "Epoch   4 Batch  856/4686   train_loss = 3.017\n",
            "Epoch   4 Batch 1056/4686   train_loss = 2.992\n",
            "Epoch   4 Batch 1256/4686   train_loss = 3.119\n",
            "Epoch   4 Batch 1456/4686   train_loss = 2.811\n",
            "Epoch   4 Batch 1656/4686   train_loss = 2.998\n",
            "Epoch   4 Batch 1856/4686   train_loss = 2.863\n",
            "Epoch   4 Batch 2056/4686   train_loss = 3.206\n",
            "Epoch   4 Batch 2256/4686   train_loss = 2.811\n",
            "Epoch   4 Batch 2456/4686   train_loss = 2.666\n",
            "Epoch   4 Batch 2656/4686   train_loss = 2.880\n",
            "Epoch   4 Batch 2856/4686   train_loss = 2.789\n",
            "Epoch   4 Batch 3056/4686   train_loss = 3.098\n",
            "Epoch   4 Batch 3256/4686   train_loss = 2.697\n",
            "Epoch   4 Batch 3456/4686   train_loss = 3.060\n",
            "Epoch   4 Batch 3656/4686   train_loss = 2.843\n",
            "Epoch   4 Batch 3856/4686   train_loss = 2.608\n",
            "Epoch   4 Batch 4056/4686   train_loss = 3.087\n",
            "Epoch   4 Batch 4256/4686   train_loss = 2.793\n",
            "Epoch   4 Batch 4456/4686   train_loss = 2.956\n",
            "Epoch   4 Batch 4656/4686   train_loss = 3.025\n",
            "Epoch   5 Batch  170/4686   train_loss = 3.112\n",
            "Epoch   5 Batch  370/4686   train_loss = 2.942\n",
            "Epoch   5 Batch  570/4686   train_loss = 2.840\n",
            "Epoch   5 Batch  770/4686   train_loss = 2.761\n",
            "Epoch   5 Batch  970/4686   train_loss = 2.595\n",
            "Epoch   5 Batch 1170/4686   train_loss = 2.783\n",
            "Epoch   5 Batch 1370/4686   train_loss = 2.653\n",
            "Epoch   5 Batch 1570/4686   train_loss = 2.920\n",
            "Epoch   5 Batch 1770/4686   train_loss = 2.978\n",
            "Epoch   5 Batch 1970/4686   train_loss = 2.844\n",
            "Epoch   5 Batch 2170/4686   train_loss = 2.603\n",
            "Epoch   5 Batch 2370/4686   train_loss = 2.750\n",
            "Epoch   5 Batch 2570/4686   train_loss = 2.926\n",
            "Epoch   5 Batch 2770/4686   train_loss = 2.656\n",
            "Epoch   5 Batch 2970/4686   train_loss = 2.803\n",
            "Epoch   5 Batch 3170/4686   train_loss = 2.886\n",
            "Epoch   5 Batch 3370/4686   train_loss = 2.694\n",
            "Epoch   5 Batch 3570/4686   train_loss = 2.900\n",
            "Epoch   5 Batch 3770/4686   train_loss = 2.958\n",
            "Epoch   5 Batch 3970/4686   train_loss = 2.561\n",
            "Epoch   5 Batch 4170/4686   train_loss = 2.619\n",
            "Epoch   5 Batch 4370/4686   train_loss = 2.869\n",
            "Epoch   5 Batch 4570/4686   train_loss = 2.887\n",
            "Epoch   6 Batch   84/4686   train_loss = 2.759\n",
            "Epoch   6 Batch  284/4686   train_loss = 2.787\n",
            "Epoch   6 Batch  484/4686   train_loss = 2.725\n",
            "Epoch   6 Batch  684/4686   train_loss = 2.674\n",
            "Epoch   6 Batch  884/4686   train_loss = 2.776\n",
            "Epoch   6 Batch 1084/4686   train_loss = 2.386\n",
            "Epoch   6 Batch 1284/4686   train_loss = 2.744\n",
            "Epoch   6 Batch 1484/4686   train_loss = 2.858\n",
            "Epoch   6 Batch 1684/4686   train_loss = 2.850\n",
            "Epoch   6 Batch 1884/4686   train_loss = 2.705\n",
            "Epoch   6 Batch 2084/4686   train_loss = 2.979\n",
            "Epoch   6 Batch 2284/4686   train_loss = 2.283\n",
            "Epoch   6 Batch 2484/4686   train_loss = 2.790\n",
            "Epoch   6 Batch 2684/4686   train_loss = 2.426\n",
            "Epoch   6 Batch 2884/4686   train_loss = 2.465\n",
            "Epoch   6 Batch 3084/4686   train_loss = 2.585\n",
            "Epoch   6 Batch 3284/4686   train_loss = 2.685\n",
            "Epoch   6 Batch 3484/4686   train_loss = 2.767\n",
            "Epoch   6 Batch 3684/4686   train_loss = 2.842\n",
            "Epoch   6 Batch 3884/4686   train_loss = 2.878\n",
            "Epoch   6 Batch 4084/4686   train_loss = 2.741\n",
            "Epoch   6 Batch 4284/4686   train_loss = 2.541\n",
            "Epoch   6 Batch 4484/4686   train_loss = 2.832\n",
            "Epoch   6 Batch 4684/4686   train_loss = 2.435\n",
            "Epoch   7 Batch  198/4686   train_loss = 2.645\n",
            "Epoch   7 Batch  398/4686   train_loss = 2.468\n",
            "Epoch   7 Batch  598/4686   train_loss = 2.607\n",
            "Epoch   7 Batch  798/4686   train_loss = 2.634\n",
            "Epoch   7 Batch  998/4686   train_loss = 2.620\n",
            "Epoch   7 Batch 1198/4686   train_loss = 2.436\n",
            "Epoch   7 Batch 1398/4686   train_loss = 2.694\n",
            "Epoch   7 Batch 1598/4686   train_loss = 2.218\n",
            "Epoch   7 Batch 1798/4686   train_loss = 2.641\n",
            "Epoch   7 Batch 1998/4686   train_loss = 2.518\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch   7 Batch 2198/4686   train_loss = 2.437\n",
            "Epoch   7 Batch 2398/4686   train_loss = 2.610\n",
            "Epoch   7 Batch 2598/4686   train_loss = 2.471\n",
            "Epoch   7 Batch 2798/4686   train_loss = 2.204\n",
            "Epoch   7 Batch 2998/4686   train_loss = 2.677\n",
            "Epoch   7 Batch 3198/4686   train_loss = 2.667\n",
            "Epoch   7 Batch 3398/4686   train_loss = 2.884\n",
            "Epoch   7 Batch 3598/4686   train_loss = 2.601\n",
            "Epoch   7 Batch 3798/4686   train_loss = 2.366\n",
            "Epoch   7 Batch 3998/4686   train_loss = 2.800\n",
            "Epoch   7 Batch 4198/4686   train_loss = 2.366\n",
            "Epoch   7 Batch 4398/4686   train_loss = 2.524\n",
            "Epoch   7 Batch 4598/4686   train_loss = 2.571\n",
            "Epoch   8 Batch  112/4686   train_loss = 2.663\n",
            "Epoch   8 Batch  312/4686   train_loss = 2.549\n",
            "Epoch   8 Batch  512/4686   train_loss = 2.690\n",
            "Epoch   8 Batch  712/4686   train_loss = 2.342\n",
            "Epoch   8 Batch  912/4686   train_loss = 2.290\n",
            "Epoch   8 Batch 1112/4686   train_loss = 2.395\n",
            "Epoch   8 Batch 1312/4686   train_loss = 2.592\n",
            "Epoch   8 Batch 1512/4686   train_loss = 2.565\n",
            "Epoch   8 Batch 1712/4686   train_loss = 2.575\n",
            "Epoch   8 Batch 1912/4686   train_loss = 2.373\n",
            "Epoch   8 Batch 2112/4686   train_loss = 2.574\n",
            "Epoch   8 Batch 2312/4686   train_loss = 2.540\n",
            "Epoch   8 Batch 2512/4686   train_loss = 2.317\n",
            "Epoch   8 Batch 2712/4686   train_loss = 2.447\n",
            "Epoch   8 Batch 2912/4686   train_loss = 2.198\n",
            "Epoch   8 Batch 3112/4686   train_loss = 2.475\n",
            "Epoch   8 Batch 3312/4686   train_loss = 2.440\n",
            "Epoch   8 Batch 3512/4686   train_loss = 2.558\n",
            "Epoch   8 Batch 3712/4686   train_loss = 2.467\n",
            "Epoch   8 Batch 3912/4686   train_loss = 2.583\n",
            "Epoch   8 Batch 4112/4686   train_loss = 2.599\n",
            "Epoch   8 Batch 4312/4686   train_loss = 2.736\n",
            "Epoch   8 Batch 4512/4686   train_loss = 2.155\n",
            "Epoch   9 Batch   26/4686   train_loss = 2.496\n",
            "Epoch   9 Batch  226/4686   train_loss = 2.578\n",
            "Epoch   9 Batch  426/4686   train_loss = 2.688\n",
            "Epoch   9 Batch  626/4686   train_loss = 2.543\n",
            "Epoch   9 Batch  826/4686   train_loss = 2.286\n",
            "Epoch   9 Batch 1026/4686   train_loss = 2.698\n",
            "Epoch   9 Batch 1226/4686   train_loss = 2.409\n",
            "Epoch   9 Batch 1426/4686   train_loss = 2.434\n",
            "Epoch   9 Batch 1626/4686   train_loss = 2.223\n",
            "Epoch   9 Batch 1826/4686   train_loss = 2.188\n",
            "Epoch   9 Batch 2026/4686   train_loss = 2.367\n",
            "Epoch   9 Batch 2226/4686   train_loss = 2.446\n",
            "Epoch   9 Batch 2426/4686   train_loss = 2.580\n",
            "Epoch   9 Batch 2626/4686   train_loss = 2.367\n",
            "Epoch   9 Batch 2826/4686   train_loss = 2.654\n",
            "Epoch   9 Batch 3026/4686   train_loss = 2.280\n",
            "Epoch   9 Batch 3226/4686   train_loss = 2.494\n",
            "Epoch   9 Batch 3426/4686   train_loss = 2.467\n",
            "Epoch   9 Batch 3626/4686   train_loss = 2.259\n",
            "Epoch   9 Batch 3826/4686   train_loss = 2.332\n",
            "Epoch   9 Batch 4026/4686   train_loss = 2.370\n",
            "Epoch   9 Batch 4226/4686   train_loss = 2.288\n",
            "Epoch   9 Batch 4426/4686   train_loss = 2.312\n",
            "Epoch   9 Batch 4626/4686   train_loss = 2.445\n",
            "Epoch  10 Batch  140/4686   train_loss = 2.331\n",
            "Epoch  10 Batch  340/4686   train_loss = 2.563\n",
            "Epoch  10 Batch  540/4686   train_loss = 2.314\n",
            "Epoch  10 Batch  740/4686   train_loss = 2.242\n",
            "Epoch  10 Batch  940/4686   train_loss = 2.293\n",
            "Epoch  10 Batch 1140/4686   train_loss = 2.295\n",
            "Epoch  10 Batch 1340/4686   train_loss = 2.360\n",
            "Epoch  10 Batch 1540/4686   train_loss = 2.568\n",
            "Epoch  10 Batch 1740/4686   train_loss = 2.226\n",
            "Epoch  10 Batch 1940/4686   train_loss = 2.515\n",
            "Epoch  10 Batch 2140/4686   train_loss = 2.217\n",
            "Epoch  10 Batch 2340/4686   train_loss = 2.066\n",
            "Epoch  10 Batch 2540/4686   train_loss = 2.345\n",
            "Epoch  10 Batch 2740/4686   train_loss = 2.390\n",
            "Epoch  10 Batch 2940/4686   train_loss = 2.313\n",
            "Epoch  10 Batch 3140/4686   train_loss = 2.302\n",
            "Epoch  10 Batch 3340/4686   train_loss = 2.417\n",
            "Epoch  10 Batch 3540/4686   train_loss = 2.313\n",
            "Epoch  10 Batch 3740/4686   train_loss = 2.538\n",
            "Epoch  10 Batch 3940/4686   train_loss = 2.426\n",
            "Epoch  10 Batch 4140/4686   train_loss = 2.267\n",
            "Epoch  10 Batch 4340/4686   train_loss = 2.457\n",
            "Epoch  10 Batch 4540/4686   train_loss = 2.292\n",
            "Epoch  11 Batch   54/4686   train_loss = 2.523\n",
            "Epoch  11 Batch  254/4686   train_loss = 2.416\n",
            "Epoch  11 Batch  454/4686   train_loss = 2.428\n",
            "Epoch  11 Batch  654/4686   train_loss = 2.232\n",
            "Epoch  11 Batch  854/4686   train_loss = 2.603\n",
            "Epoch  11 Batch 1054/4686   train_loss = 2.468\n",
            "Epoch  11 Batch 1254/4686   train_loss = 2.207\n",
            "Epoch  11 Batch 1454/4686   train_loss = 2.352\n",
            "Epoch  11 Batch 1654/4686   train_loss = 2.332\n",
            "Epoch  11 Batch 1854/4686   train_loss = 2.281\n",
            "Epoch  11 Batch 2054/4686   train_loss = 2.127\n",
            "Epoch  11 Batch 2254/4686   train_loss = 2.217\n",
            "Epoch  11 Batch 2454/4686   train_loss = 2.363\n",
            "Epoch  11 Batch 2654/4686   train_loss = 2.375\n",
            "Epoch  11 Batch 2854/4686   train_loss = 2.388\n",
            "Epoch  11 Batch 3054/4686   train_loss = 1.993\n",
            "Epoch  11 Batch 3254/4686   train_loss = 2.328\n",
            "Epoch  11 Batch 3454/4686   train_loss = 2.194\n",
            "Epoch  11 Batch 3654/4686   train_loss = 2.389\n",
            "Epoch  11 Batch 3854/4686   train_loss = 2.301\n",
            "Epoch  11 Batch 4054/4686   train_loss = 2.450\n",
            "Epoch  11 Batch 4254/4686   train_loss = 2.186\n",
            "Epoch  11 Batch 4454/4686   train_loss = 2.271\n",
            "Epoch  11 Batch 4654/4686   train_loss = 2.363\n",
            "Epoch  12 Batch  168/4686   train_loss = 2.255\n",
            "Epoch  12 Batch  368/4686   train_loss = 2.377\n",
            "Epoch  12 Batch  568/4686   train_loss = 2.235\n",
            "Epoch  12 Batch  768/4686   train_loss = 2.158\n",
            "Epoch  12 Batch  968/4686   train_loss = 2.344\n",
            "Epoch  12 Batch 1168/4686   train_loss = 2.386\n",
            "Epoch  12 Batch 1368/4686   train_loss = 2.233\n",
            "Epoch  12 Batch 1568/4686   train_loss = 2.323\n",
            "Epoch  12 Batch 1768/4686   train_loss = 2.413\n",
            "Epoch  12 Batch 1968/4686   train_loss = 2.503\n",
            "Epoch  12 Batch 2168/4686   train_loss = 2.532\n",
            "Epoch  12 Batch 2368/4686   train_loss = 2.272\n",
            "Epoch  12 Batch 2568/4686   train_loss = 2.104\n",
            "Epoch  12 Batch 2768/4686   train_loss = 2.090\n",
            "Epoch  12 Batch 2968/4686   train_loss = 2.147\n",
            "Epoch  12 Batch 3168/4686   train_loss = 2.393\n",
            "Epoch  12 Batch 3368/4686   train_loss = 2.095\n",
            "Epoch  12 Batch 3568/4686   train_loss = 2.200\n",
            "Epoch  12 Batch 3768/4686   train_loss = 2.459\n",
            "Epoch  12 Batch 3968/4686   train_loss = 2.102\n",
            "Epoch  12 Batch 4168/4686   train_loss = 2.174\n",
            "Epoch  12 Batch 4368/4686   train_loss = 2.122\n",
            "Epoch  12 Batch 4568/4686   train_loss = 2.259\n",
            "Epoch  13 Batch   82/4686   train_loss = 2.303\n",
            "Epoch  13 Batch  282/4686   train_loss = 2.298\n",
            "Epoch  13 Batch  482/4686   train_loss = 2.395\n",
            "Epoch  13 Batch  682/4686   train_loss = 2.228\n",
            "Epoch  13 Batch  882/4686   train_loss = 2.231\n",
            "Epoch  13 Batch 1082/4686   train_loss = 2.250\n",
            "Epoch  13 Batch 1282/4686   train_loss = 2.156\n",
            "Epoch  13 Batch 1482/4686   train_loss = 2.066\n",
            "Epoch  13 Batch 1682/4686   train_loss = 1.985\n",
            "Epoch  13 Batch 1882/4686   train_loss = 2.185\n",
            "Epoch  13 Batch 2082/4686   train_loss = 2.385\n",
            "Epoch  13 Batch 2282/4686   train_loss = 2.309\n",
            "Epoch  13 Batch 2482/4686   train_loss = 2.204\n",
            "Epoch  13 Batch 2682/4686   train_loss = 2.043\n",
            "Epoch  13 Batch 2882/4686   train_loss = 2.139\n",
            "Epoch  13 Batch 3082/4686   train_loss = 2.192\n",
            "Epoch  13 Batch 3282/4686   train_loss = 2.045\n",
            "Epoch  13 Batch 3482/4686   train_loss = 2.191\n",
            "Epoch  13 Batch 3682/4686   train_loss = 2.464\n",
            "Epoch  13 Batch 3882/4686   train_loss = 2.337\n",
            "Epoch  13 Batch 4082/4686   train_loss = 2.133\n",
            "Epoch  13 Batch 4282/4686   train_loss = 2.301\n",
            "Epoch  13 Batch 4482/4686   train_loss = 1.963\n",
            "Epoch  13 Batch 4682/4686   train_loss = 2.222\n",
            "Epoch  14 Batch  196/4686   train_loss = 2.284\n",
            "Epoch  14 Batch  396/4686   train_loss = 2.272\n",
            "Epoch  14 Batch  596/4686   train_loss = 2.421\n",
            "Epoch  14 Batch  796/4686   train_loss = 2.313\n",
            "Epoch  14 Batch  996/4686   train_loss = 2.212\n",
            "Epoch  14 Batch 1196/4686   train_loss = 2.415\n",
            "Epoch  14 Batch 1396/4686   train_loss = 2.195\n",
            "Epoch  14 Batch 1596/4686   train_loss = 2.082\n",
            "Epoch  14 Batch 1796/4686   train_loss = 2.285\n",
            "Epoch  14 Batch 1996/4686   train_loss = 2.042\n",
            "Epoch  14 Batch 2196/4686   train_loss = 1.905\n",
            "Epoch  14 Batch 2396/4686   train_loss = 2.188\n",
            "Epoch  14 Batch 2596/4686   train_loss = 2.275\n",
            "Epoch  14 Batch 2796/4686   train_loss = 2.454\n",
            "Epoch  14 Batch 2996/4686   train_loss = 1.987\n",
            "Epoch  14 Batch 3196/4686   train_loss = 2.003\n",
            "Epoch  14 Batch 3396/4686   train_loss = 2.032\n",
            "Epoch  14 Batch 3596/4686   train_loss = 2.016\n",
            "Epoch  14 Batch 3796/4686   train_loss = 2.134\n",
            "Epoch  14 Batch 3996/4686   train_loss = 2.299\n",
            "Epoch  14 Batch 4196/4686   train_loss = 1.938\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch  14 Batch 4396/4686   train_loss = 2.199\n",
            "Epoch  14 Batch 4596/4686   train_loss = 2.108\n",
            "Epoch  15 Batch  110/4686   train_loss = 2.044\n",
            "Epoch  15 Batch  310/4686   train_loss = 2.094\n",
            "Epoch  15 Batch  510/4686   train_loss = 2.060\n",
            "Epoch  15 Batch  710/4686   train_loss = 2.217\n",
            "Epoch  15 Batch  910/4686   train_loss = 2.135\n",
            "Epoch  15 Batch 1110/4686   train_loss = 2.166\n",
            "Epoch  15 Batch 1310/4686   train_loss = 2.141\n",
            "Epoch  15 Batch 1510/4686   train_loss = 2.303\n",
            "Epoch  15 Batch 1710/4686   train_loss = 2.112\n",
            "Epoch  15 Batch 1910/4686   train_loss = 2.196\n",
            "Epoch  15 Batch 2110/4686   train_loss = 2.263\n",
            "Epoch  15 Batch 2310/4686   train_loss = 1.863\n",
            "Epoch  15 Batch 2510/4686   train_loss = 2.081\n",
            "Epoch  15 Batch 2710/4686   train_loss = 1.972\n",
            "Epoch  15 Batch 2910/4686   train_loss = 1.937\n",
            "Epoch  15 Batch 3110/4686   train_loss = 2.243\n",
            "Epoch  15 Batch 3310/4686   train_loss = 2.013\n",
            "Epoch  15 Batch 3510/4686   train_loss = 1.986\n",
            "Epoch  15 Batch 3710/4686   train_loss = 1.891\n",
            "Epoch  15 Batch 3910/4686   train_loss = 2.357\n",
            "Epoch  15 Batch 4110/4686   train_loss = 1.911\n",
            "Epoch  15 Batch 4310/4686   train_loss = 2.356\n",
            "Epoch  15 Batch 4510/4686   train_loss = 2.226\n",
            "Epoch  16 Batch   24/4686   train_loss = 2.139\n",
            "Epoch  16 Batch  224/4686   train_loss = 2.262\n",
            "Epoch  16 Batch  424/4686   train_loss = 2.369\n",
            "Epoch  16 Batch  624/4686   train_loss = 2.350\n",
            "Epoch  16 Batch  824/4686   train_loss = 1.901\n",
            "Epoch  16 Batch 1024/4686   train_loss = 2.161\n",
            "Epoch  16 Batch 1224/4686   train_loss = 2.122\n",
            "Epoch  16 Batch 1424/4686   train_loss = 2.048\n",
            "Epoch  16 Batch 1624/4686   train_loss = 2.345\n",
            "Epoch  16 Batch 1824/4686   train_loss = 1.875\n",
            "Epoch  16 Batch 2024/4686   train_loss = 2.395\n",
            "Epoch  16 Batch 2224/4686   train_loss = 2.089\n",
            "Epoch  16 Batch 2424/4686   train_loss = 2.354\n",
            "Epoch  16 Batch 2624/4686   train_loss = 2.326\n",
            "Epoch  16 Batch 2824/4686   train_loss = 1.945\n",
            "Epoch  16 Batch 3024/4686   train_loss = 2.190\n",
            "Epoch  16 Batch 3224/4686   train_loss = 2.161\n",
            "Epoch  16 Batch 3424/4686   train_loss = 2.175\n",
            "Epoch  16 Batch 3624/4686   train_loss = 2.234\n",
            "Epoch  16 Batch 3824/4686   train_loss = 2.149\n",
            "Epoch  16 Batch 4024/4686   train_loss = 2.193\n",
            "Epoch  16 Batch 4224/4686   train_loss = 2.040\n",
            "Epoch  16 Batch 4424/4686   train_loss = 2.031\n",
            "Epoch  16 Batch 4624/4686   train_loss = 2.301\n",
            "Epoch  17 Batch  138/4686   train_loss = 2.302\n",
            "Epoch  17 Batch  338/4686   train_loss = 2.079\n",
            "Epoch  17 Batch  538/4686   train_loss = 2.179\n",
            "Epoch  17 Batch  738/4686   train_loss = 2.452\n",
            "Epoch  17 Batch  938/4686   train_loss = 2.107\n",
            "Epoch  17 Batch 1138/4686   train_loss = 2.228\n",
            "Epoch  17 Batch 1338/4686   train_loss = 2.252\n",
            "Epoch  17 Batch 1538/4686   train_loss = 2.215\n",
            "Epoch  17 Batch 1738/4686   train_loss = 2.182\n",
            "Epoch  17 Batch 1938/4686   train_loss = 2.270\n",
            "Epoch  17 Batch 2138/4686   train_loss = 2.035\n",
            "Epoch  17 Batch 2338/4686   train_loss = 2.155\n",
            "Epoch  17 Batch 2538/4686   train_loss = 1.915\n",
            "Epoch  17 Batch 2738/4686   train_loss = 2.226\n",
            "Epoch  17 Batch 2938/4686   train_loss = 2.053\n",
            "Epoch  17 Batch 3138/4686   train_loss = 2.334\n",
            "Epoch  17 Batch 3338/4686   train_loss = 1.767\n",
            "Epoch  17 Batch 3538/4686   train_loss = 2.007\n",
            "Epoch  17 Batch 3738/4686   train_loss = 2.104\n",
            "Epoch  17 Batch 3938/4686   train_loss = 1.881\n",
            "Epoch  17 Batch 4138/4686   train_loss = 1.961\n",
            "Epoch  17 Batch 4338/4686   train_loss = 2.059\n",
            "Epoch  17 Batch 4538/4686   train_loss = 2.110\n",
            "Epoch  18 Batch   52/4686   train_loss = 2.173\n",
            "Epoch  18 Batch  252/4686   train_loss = 2.247\n",
            "Epoch  18 Batch  452/4686   train_loss = 1.992\n",
            "Epoch  18 Batch  652/4686   train_loss = 2.125\n",
            "Epoch  18 Batch  852/4686   train_loss = 2.099\n",
            "Epoch  18 Batch 1052/4686   train_loss = 2.118\n",
            "Epoch  18 Batch 1252/4686   train_loss = 1.999\n",
            "Epoch  18 Batch 1452/4686   train_loss = 2.204\n",
            "Epoch  18 Batch 1652/4686   train_loss = 2.139\n",
            "Epoch  18 Batch 1852/4686   train_loss = 2.266\n",
            "Epoch  18 Batch 2052/4686   train_loss = 2.260\n",
            "Epoch  18 Batch 2252/4686   train_loss = 2.147\n",
            "Epoch  18 Batch 2452/4686   train_loss = 2.223\n",
            "Epoch  18 Batch 2652/4686   train_loss = 2.020\n",
            "Epoch  18 Batch 2852/4686   train_loss = 2.111\n",
            "Epoch  18 Batch 3052/4686   train_loss = 2.147\n",
            "Epoch  18 Batch 3252/4686   train_loss = 2.040\n",
            "Epoch  18 Batch 3452/4686   train_loss = 1.941\n",
            "Epoch  18 Batch 3652/4686   train_loss = 2.206\n",
            "Epoch  18 Batch 3852/4686   train_loss = 2.140\n",
            "Epoch  18 Batch 4052/4686   train_loss = 2.251\n",
            "Epoch  18 Batch 4252/4686   train_loss = 2.151\n",
            "Epoch  18 Batch 4452/4686   train_loss = 2.200\n",
            "Epoch  18 Batch 4652/4686   train_loss = 2.285\n",
            "Epoch  19 Batch  166/4686   train_loss = 2.401\n",
            "Epoch  19 Batch  366/4686   train_loss = 2.044\n",
            "Epoch  19 Batch  566/4686   train_loss = 2.168\n",
            "Epoch  19 Batch  766/4686   train_loss = 2.066\n",
            "Epoch  19 Batch  966/4686   train_loss = 2.157\n",
            "Epoch  19 Batch 1166/4686   train_loss = 2.385\n",
            "Epoch  19 Batch 1366/4686   train_loss = 2.145\n",
            "Epoch  19 Batch 1566/4686   train_loss = 1.803\n",
            "Epoch  19 Batch 1766/4686   train_loss = 2.074\n",
            "Epoch  19 Batch 1966/4686   train_loss = 2.016\n",
            "Epoch  19 Batch 2166/4686   train_loss = 1.797\n",
            "Epoch  19 Batch 2366/4686   train_loss = 2.278\n",
            "Epoch  19 Batch 2566/4686   train_loss = 2.160\n",
            "Epoch  19 Batch 2766/4686   train_loss = 1.999\n",
            "Epoch  19 Batch 2966/4686   train_loss = 1.908\n",
            "Epoch  19 Batch 3166/4686   train_loss = 2.282\n",
            "Epoch  19 Batch 3366/4686   train_loss = 2.169\n",
            "Epoch  19 Batch 3566/4686   train_loss = 1.957\n",
            "Epoch  19 Batch 3766/4686   train_loss = 2.131\n",
            "Epoch  19 Batch 3966/4686   train_loss = 2.043\n",
            "Epoch  19 Batch 4166/4686   train_loss = 2.149\n",
            "Epoch  19 Batch 4366/4686   train_loss = 2.070\n",
            "Epoch  19 Batch 4566/4686   train_loss = 1.991\n",
            "Epoch  20 Batch   80/4686   train_loss = 2.215\n",
            "Epoch  20 Batch  280/4686   train_loss = 1.913\n",
            "Epoch  20 Batch  480/4686   train_loss = 2.264\n",
            "Epoch  20 Batch  680/4686   train_loss = 2.214\n",
            "Epoch  20 Batch  880/4686   train_loss = 2.112\n",
            "Epoch  20 Batch 1080/4686   train_loss = 1.797\n",
            "Epoch  20 Batch 1280/4686   train_loss = 2.148\n",
            "Epoch  20 Batch 1480/4686   train_loss = 2.125\n",
            "Epoch  20 Batch 1680/4686   train_loss = 2.165\n",
            "Epoch  20 Batch 1880/4686   train_loss = 2.025\n",
            "Epoch  20 Batch 2080/4686   train_loss = 2.254\n",
            "Epoch  20 Batch 2280/4686   train_loss = 2.340\n",
            "Epoch  20 Batch 2480/4686   train_loss = 2.035\n",
            "Epoch  20 Batch 2680/4686   train_loss = 1.984\n",
            "Epoch  20 Batch 2880/4686   train_loss = 2.110\n",
            "Epoch  20 Batch 3080/4686   train_loss = 2.222\n",
            "Epoch  20 Batch 3280/4686   train_loss = 1.858\n",
            "Epoch  20 Batch 3480/4686   train_loss = 1.993\n",
            "Epoch  20 Batch 3680/4686   train_loss = 2.088\n",
            "Epoch  20 Batch 3880/4686   train_loss = 2.127\n",
            "Epoch  20 Batch 4080/4686   train_loss = 2.062\n",
            "Epoch  20 Batch 4280/4686   train_loss = 1.980\n",
            "Epoch  20 Batch 4480/4686   train_loss = 1.966\n",
            "Epoch  20 Batch 4680/4686   train_loss = 1.930\n",
            "Epoch  21 Batch  194/4686   train_loss = 2.240\n",
            "Epoch  21 Batch  394/4686   train_loss = 2.021\n",
            "Epoch  21 Batch  594/4686   train_loss = 2.065\n",
            "Epoch  21 Batch  794/4686   train_loss = 1.813\n",
            "Epoch  21 Batch  994/4686   train_loss = 2.090\n",
            "Epoch  21 Batch 1194/4686   train_loss = 1.915\n",
            "Epoch  21 Batch 1394/4686   train_loss = 1.966\n",
            "Epoch  21 Batch 1594/4686   train_loss = 1.759\n",
            "Epoch  21 Batch 1794/4686   train_loss = 2.180\n",
            "Epoch  21 Batch 1994/4686   train_loss = 2.227\n",
            "Epoch  21 Batch 2194/4686   train_loss = 2.017\n",
            "Epoch  21 Batch 2394/4686   train_loss = 2.062\n",
            "Epoch  21 Batch 2594/4686   train_loss = 1.901\n",
            "Epoch  21 Batch 2794/4686   train_loss = 1.883\n",
            "Epoch  21 Batch 2994/4686   train_loss = 2.154\n",
            "Epoch  21 Batch 3194/4686   train_loss = 2.157\n",
            "Epoch  21 Batch 3394/4686   train_loss = 2.146\n",
            "Epoch  21 Batch 3594/4686   train_loss = 2.185\n",
            "Epoch  21 Batch 3794/4686   train_loss = 2.208\n",
            "Epoch  21 Batch 3994/4686   train_loss = 2.108\n",
            "Epoch  21 Batch 4194/4686   train_loss = 2.148\n",
            "Epoch  21 Batch 4394/4686   train_loss = 1.835\n",
            "Epoch  21 Batch 4594/4686   train_loss = 2.175\n",
            "Epoch  22 Batch  108/4686   train_loss = 2.199\n",
            "Epoch  22 Batch  308/4686   train_loss = 2.187\n",
            "Epoch  22 Batch  508/4686   train_loss = 2.190\n",
            "Epoch  22 Batch  708/4686   train_loss = 1.815\n",
            "Epoch  22 Batch  908/4686   train_loss = 1.945\n",
            "Epoch  22 Batch 1108/4686   train_loss = 2.002\n",
            "Epoch  22 Batch 1308/4686   train_loss = 1.893\n",
            "Epoch  22 Batch 1508/4686   train_loss = 1.596\n",
            "Epoch  22 Batch 1708/4686   train_loss = 2.112\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch  22 Batch 1908/4686   train_loss = 2.077\n",
            "Epoch  22 Batch 2108/4686   train_loss = 2.041\n",
            "Epoch  22 Batch 2308/4686   train_loss = 2.205\n",
            "Epoch  22 Batch 2508/4686   train_loss = 1.812\n",
            "Epoch  22 Batch 2708/4686   train_loss = 2.365\n",
            "Epoch  22 Batch 2908/4686   train_loss = 1.887\n",
            "Epoch  22 Batch 3108/4686   train_loss = 2.128\n",
            "Epoch  22 Batch 3308/4686   train_loss = 2.156\n",
            "Epoch  22 Batch 3508/4686   train_loss = 2.156\n",
            "Epoch  22 Batch 3708/4686   train_loss = 2.089\n",
            "Epoch  22 Batch 3908/4686   train_loss = 2.139\n",
            "Epoch  22 Batch 4108/4686   train_loss = 2.179\n",
            "Epoch  22 Batch 4308/4686   train_loss = 2.265\n",
            "Epoch  22 Batch 4508/4686   train_loss = 1.880\n",
            "Epoch  23 Batch   22/4686   train_loss = 2.191\n",
            "Epoch  23 Batch  222/4686   train_loss = 2.148\n",
            "Epoch  23 Batch  422/4686   train_loss = 2.007\n",
            "Epoch  23 Batch  622/4686   train_loss = 2.034\n",
            "Epoch  23 Batch  822/4686   train_loss = 2.188\n",
            "Epoch  23 Batch 1022/4686   train_loss = 2.073\n",
            "Epoch  23 Batch 1222/4686   train_loss = 2.036\n",
            "Epoch  23 Batch 1422/4686   train_loss = 1.896\n",
            "Epoch  23 Batch 1622/4686   train_loss = 2.123\n",
            "Epoch  23 Batch 1822/4686   train_loss = 2.304\n",
            "Epoch  23 Batch 2022/4686   train_loss = 1.945\n",
            "Epoch  23 Batch 2222/4686   train_loss = 2.135\n",
            "Epoch  23 Batch 2422/4686   train_loss = 2.146\n",
            "Epoch  23 Batch 2622/4686   train_loss = 2.161\n",
            "Epoch  23 Batch 2822/4686   train_loss = 2.139\n",
            "Epoch  23 Batch 3022/4686   train_loss = 1.766\n",
            "Epoch  23 Batch 3222/4686   train_loss = 2.074\n",
            "Epoch  23 Batch 3422/4686   train_loss = 2.112\n",
            "Epoch  23 Batch 3622/4686   train_loss = 2.092\n",
            "Epoch  23 Batch 3822/4686   train_loss = 2.116\n",
            "Epoch  23 Batch 4022/4686   train_loss = 2.100\n",
            "Epoch  23 Batch 4222/4686   train_loss = 1.985\n",
            "Epoch  23 Batch 4422/4686   train_loss = 1.961\n",
            "Epoch  23 Batch 4622/4686   train_loss = 2.009\n",
            "Epoch  24 Batch  136/4686   train_loss = 2.063\n",
            "Epoch  24 Batch  336/4686   train_loss = 2.087\n",
            "Epoch  24 Batch  536/4686   train_loss = 1.933\n",
            "Epoch  24 Batch  736/4686   train_loss = 1.962\n",
            "Epoch  24 Batch  936/4686   train_loss = 1.989\n",
            "Epoch  24 Batch 1136/4686   train_loss = 2.329\n",
            "Epoch  24 Batch 1336/4686   train_loss = 2.134\n",
            "Epoch  24 Batch 1536/4686   train_loss = 1.781\n",
            "Epoch  24 Batch 1736/4686   train_loss = 2.083\n",
            "Epoch  24 Batch 1936/4686   train_loss = 2.180\n",
            "Epoch  24 Batch 2136/4686   train_loss = 1.720\n",
            "Epoch  24 Batch 2336/4686   train_loss = 2.071\n",
            "Epoch  24 Batch 2536/4686   train_loss = 1.913\n",
            "Epoch  24 Batch 2736/4686   train_loss = 1.911\n",
            "Epoch  24 Batch 2936/4686   train_loss = 2.062\n",
            "Epoch  24 Batch 3136/4686   train_loss = 1.861\n",
            "Epoch  24 Batch 3336/4686   train_loss = 2.001\n",
            "Epoch  24 Batch 3536/4686   train_loss = 1.968\n",
            "Epoch  24 Batch 3736/4686   train_loss = 2.063\n",
            "Epoch  24 Batch 3936/4686   train_loss = 1.803\n",
            "Epoch  24 Batch 4136/4686   train_loss = 2.079\n",
            "Epoch  24 Batch 4336/4686   train_loss = 2.259\n",
            "Epoch  24 Batch 4536/4686   train_loss = 1.748\n",
            "Epoch  25 Batch   50/4686   train_loss = 2.232\n",
            "Epoch  25 Batch  250/4686   train_loss = 2.224\n",
            "Epoch  25 Batch  450/4686   train_loss = 2.220\n",
            "Epoch  25 Batch  650/4686   train_loss = 1.948\n",
            "Epoch  25 Batch  850/4686   train_loss = 1.963\n",
            "Epoch  25 Batch 1050/4686   train_loss = 1.988\n",
            "Epoch  25 Batch 1250/4686   train_loss = 2.004\n",
            "Epoch  25 Batch 1450/4686   train_loss = 1.918\n",
            "Epoch  25 Batch 1650/4686   train_loss = 1.892\n",
            "Epoch  25 Batch 1850/4686   train_loss = 1.841\n",
            "Epoch  25 Batch 2050/4686   train_loss = 2.053\n",
            "Epoch  25 Batch 2250/4686   train_loss = 2.098\n",
            "Epoch  25 Batch 2450/4686   train_loss = 1.918\n",
            "Epoch  25 Batch 2650/4686   train_loss = 2.142\n",
            "Epoch  25 Batch 2850/4686   train_loss = 1.759\n",
            "Epoch  25 Batch 3050/4686   train_loss = 2.188\n",
            "Epoch  25 Batch 3250/4686   train_loss = 1.873\n",
            "Epoch  25 Batch 3450/4686   train_loss = 1.862\n",
            "Epoch  25 Batch 3650/4686   train_loss = 1.975\n",
            "Epoch  25 Batch 3850/4686   train_loss = 2.229\n",
            "Epoch  25 Batch 4050/4686   train_loss = 1.959\n",
            "Epoch  25 Batch 4250/4686   train_loss = 1.921\n",
            "Epoch  25 Batch 4450/4686   train_loss = 1.829\n",
            "Epoch  25 Batch 4650/4686   train_loss = 2.053\n",
            "Epoch  26 Batch  164/4686   train_loss = 2.137\n",
            "Epoch  26 Batch  364/4686   train_loss = 1.738\n",
            "Epoch  26 Batch  564/4686   train_loss = 1.924\n",
            "Epoch  26 Batch  764/4686   train_loss = 2.030\n",
            "Epoch  26 Batch  964/4686   train_loss = 2.119\n",
            "Epoch  26 Batch 1164/4686   train_loss = 1.979\n",
            "Epoch  26 Batch 1364/4686   train_loss = 1.843\n",
            "Epoch  26 Batch 1564/4686   train_loss = 1.972\n",
            "Epoch  26 Batch 1764/4686   train_loss = 1.972\n",
            "Epoch  26 Batch 1964/4686   train_loss = 1.917\n",
            "Epoch  26 Batch 2164/4686   train_loss = 1.683\n",
            "Epoch  26 Batch 2364/4686   train_loss = 2.102\n",
            "Epoch  26 Batch 2564/4686   train_loss = 2.341\n",
            "Epoch  26 Batch 2764/4686   train_loss = 1.897\n",
            "Epoch  26 Batch 2964/4686   train_loss = 1.688\n",
            "Epoch  26 Batch 3164/4686   train_loss = 2.220\n",
            "Epoch  26 Batch 3364/4686   train_loss = 1.894\n",
            "Epoch  26 Batch 3564/4686   train_loss = 2.002\n",
            "Epoch  26 Batch 3764/4686   train_loss = 1.970\n",
            "Epoch  26 Batch 3964/4686   train_loss = 2.026\n",
            "Epoch  26 Batch 4164/4686   train_loss = 1.931\n",
            "Epoch  26 Batch 4364/4686   train_loss = 2.261\n",
            "Epoch  26 Batch 4564/4686   train_loss = 2.094\n",
            "Epoch  27 Batch   78/4686   train_loss = 2.299\n",
            "Epoch  27 Batch  278/4686   train_loss = 2.052\n",
            "Epoch  27 Batch  478/4686   train_loss = 1.964\n",
            "Epoch  27 Batch  678/4686   train_loss = 2.100\n",
            "Epoch  27 Batch  878/4686   train_loss = 1.906\n",
            "Epoch  27 Batch 1078/4686   train_loss = 1.961\n",
            "Epoch  27 Batch 1278/4686   train_loss = 1.830\n",
            "Epoch  27 Batch 1478/4686   train_loss = 1.606\n",
            "Epoch  27 Batch 1678/4686   train_loss = 1.834\n",
            "Epoch  27 Batch 1878/4686   train_loss = 2.270\n",
            "Epoch  27 Batch 2078/4686   train_loss = 2.098\n",
            "Epoch  27 Batch 2278/4686   train_loss = 2.014\n",
            "Epoch  27 Batch 2478/4686   train_loss = 2.155\n",
            "Epoch  27 Batch 2678/4686   train_loss = 1.781\n",
            "Epoch  27 Batch 2878/4686   train_loss = 1.754\n",
            "Epoch  27 Batch 3078/4686   train_loss = 1.731\n",
            "Epoch  27 Batch 3278/4686   train_loss = 1.856\n",
            "Epoch  27 Batch 3478/4686   train_loss = 2.082\n",
            "Epoch  27 Batch 3678/4686   train_loss = 1.989\n",
            "Epoch  27 Batch 3878/4686   train_loss = 2.102\n",
            "Epoch  27 Batch 4078/4686   train_loss = 2.178\n",
            "Epoch  27 Batch 4278/4686   train_loss = 1.895\n",
            "Epoch  27 Batch 4478/4686   train_loss = 2.089\n",
            "Epoch  27 Batch 4678/4686   train_loss = 2.230\n",
            "Epoch  28 Batch  192/4686   train_loss = 2.179\n",
            "Epoch  28 Batch  392/4686   train_loss = 2.030\n",
            "Epoch  28 Batch  592/4686   train_loss = 2.200\n",
            "Epoch  28 Batch  792/4686   train_loss = 2.018\n",
            "Epoch  28 Batch  992/4686   train_loss = 2.041\n",
            "Epoch  28 Batch 1192/4686   train_loss = 1.705\n",
            "Epoch  28 Batch 1392/4686   train_loss = 1.677\n",
            "Epoch  28 Batch 1592/4686   train_loss = 1.873\n",
            "Epoch  28 Batch 1792/4686   train_loss = 1.832\n",
            "Epoch  28 Batch 1992/4686   train_loss = 2.073\n",
            "Epoch  28 Batch 2192/4686   train_loss = 2.064\n",
            "Epoch  28 Batch 2392/4686   train_loss = 1.892\n",
            "Epoch  28 Batch 2592/4686   train_loss = 1.933\n",
            "Epoch  28 Batch 2792/4686   train_loss = 1.873\n",
            "Epoch  28 Batch 2992/4686   train_loss = 1.705\n",
            "Epoch  28 Batch 3192/4686   train_loss = 1.916\n",
            "Epoch  28 Batch 3392/4686   train_loss = 1.953\n",
            "Epoch  28 Batch 3592/4686   train_loss = 1.891\n",
            "Epoch  28 Batch 3792/4686   train_loss = 1.682\n",
            "Epoch  28 Batch 3992/4686   train_loss = 1.980\n",
            "Epoch  28 Batch 4192/4686   train_loss = 1.889\n",
            "Epoch  28 Batch 4392/4686   train_loss = 2.063\n",
            "Epoch  28 Batch 4592/4686   train_loss = 1.998\n",
            "Epoch  29 Batch  106/4686   train_loss = 1.751\n",
            "Epoch  29 Batch  306/4686   train_loss = 2.009\n",
            "Epoch  29 Batch  506/4686   train_loss = 1.882\n",
            "Epoch  29 Batch  706/4686   train_loss = 1.771\n",
            "Epoch  29 Batch  906/4686   train_loss = 1.941\n",
            "Epoch  29 Batch 1106/4686   train_loss = 2.060\n",
            "Epoch  29 Batch 1306/4686   train_loss = 2.018\n",
            "Epoch  29 Batch 1506/4686   train_loss = 1.939\n",
            "Epoch  29 Batch 1706/4686   train_loss = 1.734\n",
            "Epoch  29 Batch 1906/4686   train_loss = 1.839\n",
            "Epoch  29 Batch 2106/4686   train_loss = 2.075\n",
            "Epoch  29 Batch 2306/4686   train_loss = 2.209\n",
            "Epoch  29 Batch 2506/4686   train_loss = 2.111\n",
            "Epoch  29 Batch 2706/4686   train_loss = 1.828\n",
            "Epoch  29 Batch 2906/4686   train_loss = 1.942\n",
            "Epoch  29 Batch 3106/4686   train_loss = 1.943\n",
            "Epoch  29 Batch 3306/4686   train_loss = 1.982\n",
            "Epoch  29 Batch 3506/4686   train_loss = 1.913\n",
            "Epoch  29 Batch 3706/4686   train_loss = 2.113\n",
            "Epoch  29 Batch 3906/4686   train_loss = 2.182\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch  29 Batch 4106/4686   train_loss = 2.083\n",
            "Epoch  29 Batch 4306/4686   train_loss = 1.937\n",
            "Epoch  29 Batch 4506/4686   train_loss = 2.148\n",
            "Epoch  30 Batch   20/4686   train_loss = 1.816\n",
            "Epoch  30 Batch  220/4686   train_loss = 1.967\n",
            "Epoch  30 Batch  420/4686   train_loss = 1.988\n",
            "Epoch  30 Batch  620/4686   train_loss = 2.027\n",
            "Epoch  30 Batch  820/4686   train_loss = 2.017\n",
            "Epoch  30 Batch 1020/4686   train_loss = 1.906\n",
            "Epoch  30 Batch 1220/4686   train_loss = 1.750\n",
            "Epoch  30 Batch 1420/4686   train_loss = 1.890\n",
            "Epoch  30 Batch 1620/4686   train_loss = 1.962\n",
            "Epoch  30 Batch 1820/4686   train_loss = 1.966\n",
            "Epoch  30 Batch 2020/4686   train_loss = 1.941\n",
            "Epoch  30 Batch 2220/4686   train_loss = 1.876\n",
            "Epoch  30 Batch 2420/4686   train_loss = 1.946\n",
            "Epoch  30 Batch 2620/4686   train_loss = 1.862\n",
            "Epoch  30 Batch 2820/4686   train_loss = 2.046\n",
            "Epoch  30 Batch 3020/4686   train_loss = 1.737\n",
            "Epoch  30 Batch 3220/4686   train_loss = 1.847\n",
            "Epoch  30 Batch 3420/4686   train_loss = 2.126\n",
            "Epoch  30 Batch 3620/4686   train_loss = 2.053\n",
            "Epoch  30 Batch 3820/4686   train_loss = 1.989\n",
            "Epoch  30 Batch 4020/4686   train_loss = 1.802\n",
            "Epoch  30 Batch 4220/4686   train_loss = 1.941\n",
            "Epoch  30 Batch 4420/4686   train_loss = 1.714\n",
            "Epoch  30 Batch 4620/4686   train_loss = 1.995\n",
            "Epoch  31 Batch  134/4686   train_loss = 2.100\n",
            "Epoch  31 Batch  334/4686   train_loss = 1.955\n",
            "Epoch  31 Batch  534/4686   train_loss = 1.826\n",
            "Epoch  31 Batch  734/4686   train_loss = 1.659\n",
            "Epoch  31 Batch  934/4686   train_loss = 1.892\n",
            "Epoch  31 Batch 1134/4686   train_loss = 2.019\n",
            "Epoch  31 Batch 1334/4686   train_loss = 1.870\n",
            "Epoch  31 Batch 1534/4686   train_loss = 2.057\n",
            "Epoch  31 Batch 1734/4686   train_loss = 2.166\n",
            "Epoch  31 Batch 1934/4686   train_loss = 1.952\n",
            "Epoch  31 Batch 2134/4686   train_loss = 1.832\n",
            "Epoch  31 Batch 2334/4686   train_loss = 1.805\n",
            "Epoch  31 Batch 2534/4686   train_loss = 2.194\n",
            "Epoch  31 Batch 2734/4686   train_loss = 2.132\n",
            "Epoch  31 Batch 2934/4686   train_loss = 1.847\n",
            "Epoch  31 Batch 3134/4686   train_loss = 2.012\n",
            "Epoch  31 Batch 3334/4686   train_loss = 2.049\n",
            "Epoch  31 Batch 3534/4686   train_loss = 2.000\n",
            "Epoch  31 Batch 3734/4686   train_loss = 1.958\n",
            "Epoch  31 Batch 3934/4686   train_loss = 2.093\n",
            "Epoch  31 Batch 4134/4686   train_loss = 2.055\n",
            "Epoch  31 Batch 4334/4686   train_loss = 1.927\n",
            "Epoch  31 Batch 4534/4686   train_loss = 2.161\n",
            "Epoch  32 Batch   48/4686   train_loss = 2.012\n",
            "Epoch  32 Batch  248/4686   train_loss = 1.936\n",
            "Epoch  32 Batch  448/4686   train_loss = 1.998\n",
            "Epoch  32 Batch  648/4686   train_loss = 1.682\n",
            "Epoch  32 Batch  848/4686   train_loss = 2.094\n",
            "Epoch  32 Batch 1048/4686   train_loss = 2.104\n",
            "Epoch  32 Batch 1248/4686   train_loss = 2.069\n",
            "Epoch  32 Batch 1448/4686   train_loss = 1.867\n",
            "Epoch  32 Batch 1648/4686   train_loss = 1.676\n",
            "Epoch  32 Batch 1848/4686   train_loss = 1.938\n",
            "Epoch  32 Batch 2048/4686   train_loss = 2.075\n",
            "Epoch  32 Batch 2248/4686   train_loss = 2.119\n",
            "Epoch  32 Batch 2448/4686   train_loss = 2.064\n",
            "Epoch  32 Batch 2648/4686   train_loss = 1.999\n",
            "Epoch  32 Batch 2848/4686   train_loss = 1.710\n",
            "Epoch  32 Batch 3048/4686   train_loss = 1.853\n",
            "Epoch  32 Batch 3248/4686   train_loss = 1.979\n",
            "Epoch  32 Batch 3448/4686   train_loss = 1.889\n",
            "Epoch  32 Batch 3648/4686   train_loss = 2.079\n",
            "Epoch  32 Batch 3848/4686   train_loss = 1.781\n",
            "Epoch  32 Batch 4048/4686   train_loss = 2.135\n",
            "Epoch  32 Batch 4248/4686   train_loss = 1.970\n",
            "Epoch  32 Batch 4448/4686   train_loss = 2.007\n",
            "Epoch  32 Batch 4648/4686   train_loss = 2.051\n",
            "Epoch  33 Batch  162/4686   train_loss = 1.961\n",
            "Epoch  33 Batch  362/4686   train_loss = 1.807\n",
            "Epoch  33 Batch  562/4686   train_loss = 1.771\n",
            "Epoch  33 Batch  762/4686   train_loss = 1.955\n",
            "Epoch  33 Batch  962/4686   train_loss = 1.922\n",
            "Epoch  33 Batch 1162/4686   train_loss = 1.772\n",
            "Epoch  33 Batch 1362/4686   train_loss = 1.908\n",
            "Epoch  33 Batch 1562/4686   train_loss = 2.196\n",
            "Epoch  33 Batch 1762/4686   train_loss = 2.008\n",
            "Epoch  33 Batch 1962/4686   train_loss = 1.935\n",
            "Epoch  33 Batch 2162/4686   train_loss = 2.031\n",
            "Epoch  33 Batch 2362/4686   train_loss = 1.864\n",
            "Epoch  33 Batch 2562/4686   train_loss = 1.710\n",
            "Epoch  33 Batch 2762/4686   train_loss = 1.677\n",
            "Epoch  33 Batch 2962/4686   train_loss = 2.051\n",
            "Epoch  33 Batch 3162/4686   train_loss = 1.893\n",
            "Epoch  33 Batch 3362/4686   train_loss = 1.869\n",
            "Epoch  33 Batch 3562/4686   train_loss = 1.855\n",
            "Epoch  33 Batch 3762/4686   train_loss = 2.111\n",
            "Epoch  33 Batch 3962/4686   train_loss = 2.077\n",
            "Epoch  33 Batch 4162/4686   train_loss = 1.866\n",
            "Epoch  33 Batch 4362/4686   train_loss = 1.803\n",
            "Epoch  33 Batch 4562/4686   train_loss = 2.132\n",
            "Epoch  34 Batch   76/4686   train_loss = 2.062\n",
            "Epoch  34 Batch  276/4686   train_loss = 2.159\n",
            "Epoch  34 Batch  476/4686   train_loss = 1.953\n",
            "Epoch  34 Batch  676/4686   train_loss = 2.140\n",
            "Epoch  34 Batch  876/4686   train_loss = 1.795\n",
            "Epoch  34 Batch 1076/4686   train_loss = 2.012\n",
            "Epoch  34 Batch 1276/4686   train_loss = 1.644\n",
            "Epoch  34 Batch 1476/4686   train_loss = 1.877\n",
            "Epoch  34 Batch 1676/4686   train_loss = 1.946\n",
            "Epoch  34 Batch 1876/4686   train_loss = 1.998\n",
            "Epoch  34 Batch 2076/4686   train_loss = 2.155\n",
            "Epoch  34 Batch 2276/4686   train_loss = 2.043\n",
            "Epoch  34 Batch 2476/4686   train_loss = 1.914\n",
            "Epoch  34 Batch 2676/4686   train_loss = 1.949\n",
            "Epoch  34 Batch 2876/4686   train_loss = 1.812\n",
            "Epoch  34 Batch 3076/4686   train_loss = 2.114\n",
            "Epoch  34 Batch 3276/4686   train_loss = 1.756\n",
            "Epoch  34 Batch 3476/4686   train_loss = 1.957\n",
            "Epoch  34 Batch 3676/4686   train_loss = 1.900\n",
            "Epoch  34 Batch 3876/4686   train_loss = 1.938\n",
            "Epoch  34 Batch 4076/4686   train_loss = 1.842\n",
            "Epoch  34 Batch 4276/4686   train_loss = 1.962\n",
            "Epoch  34 Batch 4476/4686   train_loss = 1.810\n",
            "Epoch  34 Batch 4676/4686   train_loss = 1.809\n",
            "Epoch  35 Batch  190/4686   train_loss = 2.152\n",
            "Epoch  35 Batch  390/4686   train_loss = 1.791\n",
            "Epoch  35 Batch  590/4686   train_loss = 1.925\n",
            "Epoch  35 Batch  790/4686   train_loss = 2.038\n",
            "Epoch  35 Batch  990/4686   train_loss = 1.934\n",
            "Epoch  35 Batch 1190/4686   train_loss = 1.822\n",
            "Epoch  35 Batch 1390/4686   train_loss = 2.016\n",
            "Epoch  35 Batch 1590/4686   train_loss = 1.564\n",
            "Epoch  35 Batch 1790/4686   train_loss = 1.983\n",
            "Epoch  35 Batch 1990/4686   train_loss = 1.799\n",
            "Epoch  35 Batch 2190/4686   train_loss = 1.850\n",
            "Epoch  35 Batch 2390/4686   train_loss = 1.667\n",
            "Epoch  35 Batch 2590/4686   train_loss = 1.967\n",
            "Epoch  35 Batch 2790/4686   train_loss = 1.808\n",
            "Epoch  35 Batch 2990/4686   train_loss = 1.930\n",
            "Epoch  35 Batch 3190/4686   train_loss = 1.848\n",
            "Epoch  35 Batch 3390/4686   train_loss = 1.951\n",
            "Epoch  35 Batch 3590/4686   train_loss = 1.829\n",
            "Epoch  35 Batch 3790/4686   train_loss = 1.995\n",
            "Epoch  35 Batch 3990/4686   train_loss = 1.978\n",
            "Epoch  35 Batch 4190/4686   train_loss = 1.693\n",
            "Epoch  35 Batch 4390/4686   train_loss = 2.008\n",
            "Epoch  35 Batch 4590/4686   train_loss = 2.243\n",
            "Epoch  36 Batch  104/4686   train_loss = 1.741\n",
            "Epoch  36 Batch  304/4686   train_loss = 2.172\n",
            "Epoch  36 Batch  504/4686   train_loss = 1.969\n",
            "Epoch  36 Batch  704/4686   train_loss = 1.997\n",
            "Epoch  36 Batch  904/4686   train_loss = 1.916\n",
            "Epoch  36 Batch 1104/4686   train_loss = 1.881\n",
            "Epoch  36 Batch 1304/4686   train_loss = 1.690\n",
            "Epoch  36 Batch 1504/4686   train_loss = 1.787\n",
            "Epoch  36 Batch 1704/4686   train_loss = 1.984\n",
            "Epoch  36 Batch 1904/4686   train_loss = 1.853\n",
            "Epoch  36 Batch 2104/4686   train_loss = 2.056\n",
            "Epoch  36 Batch 2304/4686   train_loss = 1.868\n",
            "Epoch  36 Batch 2504/4686   train_loss = 1.679\n",
            "Epoch  36 Batch 2704/4686   train_loss = 1.917\n",
            "Epoch  36 Batch 2904/4686   train_loss = 2.048\n",
            "Epoch  36 Batch 3104/4686   train_loss = 1.888\n",
            "Epoch  36 Batch 3304/4686   train_loss = 1.965\n",
            "Epoch  36 Batch 3504/4686   train_loss = 2.198\n",
            "Epoch  36 Batch 3704/4686   train_loss = 2.023\n",
            "Epoch  36 Batch 3904/4686   train_loss = 1.839\n",
            "Epoch  36 Batch 4104/4686   train_loss = 1.565\n",
            "Epoch  36 Batch 4304/4686   train_loss = 1.917\n",
            "Epoch  36 Batch 4504/4686   train_loss = 1.987\n",
            "Epoch  37 Batch   18/4686   train_loss = 1.831\n",
            "Epoch  37 Batch  218/4686   train_loss = 1.950\n",
            "Epoch  37 Batch  418/4686   train_loss = 1.911\n",
            "Epoch  37 Batch  618/4686   train_loss = 1.890\n",
            "Epoch  37 Batch  818/4686   train_loss = 2.192\n",
            "Epoch  37 Batch 1018/4686   train_loss = 1.807\n",
            "Epoch  37 Batch 1218/4686   train_loss = 1.572\n",
            "Epoch  37 Batch 1418/4686   train_loss = 2.071\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch  37 Batch 1618/4686   train_loss = 1.850\n",
            "Epoch  37 Batch 1818/4686   train_loss = 1.947\n",
            "Epoch  37 Batch 2018/4686   train_loss = 1.811\n",
            "Epoch  37 Batch 2218/4686   train_loss = 1.859\n",
            "Epoch  37 Batch 2418/4686   train_loss = 1.826\n",
            "Epoch  37 Batch 2618/4686   train_loss = 2.134\n",
            "Epoch  37 Batch 2818/4686   train_loss = 2.044\n",
            "Epoch  37 Batch 3018/4686   train_loss = 1.722\n",
            "Epoch  37 Batch 3218/4686   train_loss = 1.736\n",
            "Epoch  37 Batch 3418/4686   train_loss = 1.937\n",
            "Epoch  37 Batch 3618/4686   train_loss = 1.751\n",
            "Epoch  37 Batch 3818/4686   train_loss = 1.811\n",
            "Epoch  37 Batch 4018/4686   train_loss = 1.961\n",
            "Epoch  37 Batch 4218/4686   train_loss = 1.934\n",
            "Epoch  37 Batch 4418/4686   train_loss = 1.988\n",
            "Epoch  37 Batch 4618/4686   train_loss = 1.920\n",
            "Epoch  38 Batch  132/4686   train_loss = 1.928\n",
            "Epoch  38 Batch  332/4686   train_loss = 1.814\n",
            "Epoch  38 Batch  532/4686   train_loss = 1.988\n",
            "Epoch  38 Batch  732/4686   train_loss = 1.671\n",
            "Epoch  38 Batch  932/4686   train_loss = 1.768\n",
            "Epoch  38 Batch 1132/4686   train_loss = 1.698\n",
            "Epoch  38 Batch 1332/4686   train_loss = 1.742\n",
            "Epoch  38 Batch 1532/4686   train_loss = 1.988\n",
            "Epoch  38 Batch 1732/4686   train_loss = 1.908\n",
            "Epoch  38 Batch 1932/4686   train_loss = 1.976\n",
            "Epoch  38 Batch 2132/4686   train_loss = 1.744\n",
            "Epoch  38 Batch 2332/4686   train_loss = 1.907\n",
            "Epoch  38 Batch 2532/4686   train_loss = 1.821\n",
            "Epoch  38 Batch 2732/4686   train_loss = 1.773\n",
            "Epoch  38 Batch 2932/4686   train_loss = 1.795\n",
            "Epoch  38 Batch 3132/4686   train_loss = 1.930\n",
            "Epoch  38 Batch 3332/4686   train_loss = 2.009\n",
            "Epoch  38 Batch 3532/4686   train_loss = 2.038\n",
            "Epoch  38 Batch 3732/4686   train_loss = 1.957\n",
            "Epoch  38 Batch 3932/4686   train_loss = 1.747\n",
            "Epoch  38 Batch 4132/4686   train_loss = 2.159\n",
            "Epoch  38 Batch 4332/4686   train_loss = 2.125\n",
            "Epoch  38 Batch 4532/4686   train_loss = 2.017\n",
            "Epoch  39 Batch   46/4686   train_loss = 1.899\n",
            "Epoch  39 Batch  246/4686   train_loss = 2.139\n",
            "Epoch  39 Batch  446/4686   train_loss = 1.862\n",
            "Epoch  39 Batch  646/4686   train_loss = 1.853\n",
            "Epoch  39 Batch  846/4686   train_loss = 1.888\n",
            "Epoch  39 Batch 1046/4686   train_loss = 1.813\n",
            "Epoch  39 Batch 1246/4686   train_loss = 1.950\n",
            "Epoch  39 Batch 1446/4686   train_loss = 1.870\n",
            "Epoch  39 Batch 1646/4686   train_loss = 2.083\n",
            "Epoch  39 Batch 1846/4686   train_loss = 1.957\n",
            "Epoch  39 Batch 2046/4686   train_loss = 1.848\n",
            "Epoch  39 Batch 2246/4686   train_loss = 1.883\n",
            "Epoch  39 Batch 2446/4686   train_loss = 1.868\n",
            "Epoch  39 Batch 2646/4686   train_loss = 1.849\n",
            "Epoch  39 Batch 2846/4686   train_loss = 1.745\n",
            "Epoch  39 Batch 3046/4686   train_loss = 1.901\n",
            "Epoch  39 Batch 3246/4686   train_loss = 2.240\n",
            "Epoch  39 Batch 3446/4686   train_loss = 1.786\n",
            "Epoch  39 Batch 3646/4686   train_loss = 1.630\n",
            "Epoch  39 Batch 3846/4686   train_loss = 2.172\n",
            "Epoch  39 Batch 4046/4686   train_loss = 1.833\n",
            "Epoch  39 Batch 4246/4686   train_loss = 1.861\n",
            "Epoch  39 Batch 4446/4686   train_loss = 1.983\n",
            "Epoch  39 Batch 4646/4686   train_loss = 2.150\n",
            "Epoch  40 Batch  160/4686   train_loss = 1.842\n",
            "Epoch  40 Batch  360/4686   train_loss = 1.837\n",
            "Epoch  40 Batch  560/4686   train_loss = 1.900\n",
            "Epoch  40 Batch  760/4686   train_loss = 1.831\n",
            "Epoch  40 Batch  960/4686   train_loss = 1.828\n",
            "Epoch  40 Batch 1160/4686   train_loss = 1.830\n",
            "Epoch  40 Batch 1360/4686   train_loss = 1.829\n",
            "Epoch  40 Batch 1560/4686   train_loss = 1.982\n",
            "Epoch  40 Batch 1760/4686   train_loss = 1.715\n",
            "Epoch  40 Batch 1960/4686   train_loss = 1.909\n",
            "Epoch  40 Batch 2160/4686   train_loss = 1.608\n",
            "Epoch  40 Batch 2360/4686   train_loss = 1.620\n",
            "Epoch  40 Batch 2560/4686   train_loss = 2.073\n",
            "Epoch  40 Batch 2760/4686   train_loss = 1.997\n",
            "Epoch  40 Batch 2960/4686   train_loss = 1.778\n",
            "Epoch  40 Batch 3160/4686   train_loss = 1.915\n",
            "Epoch  40 Batch 3360/4686   train_loss = 1.835\n",
            "Epoch  40 Batch 3560/4686   train_loss = 1.915\n",
            "Epoch  40 Batch 3760/4686   train_loss = 1.709\n",
            "Epoch  40 Batch 3960/4686   train_loss = 1.871\n",
            "Epoch  40 Batch 4160/4686   train_loss = 1.912\n",
            "Epoch  40 Batch 4360/4686   train_loss = 2.113\n",
            "Epoch  40 Batch 4560/4686   train_loss = 1.801\n",
            "Epoch  41 Batch   74/4686   train_loss = 1.594\n",
            "Epoch  41 Batch  274/4686   train_loss = 1.941\n",
            "Epoch  41 Batch  474/4686   train_loss = 1.873\n",
            "Epoch  41 Batch  674/4686   train_loss = 1.911\n",
            "Epoch  41 Batch  874/4686   train_loss = 2.045\n",
            "Epoch  41 Batch 1074/4686   train_loss = 1.783\n",
            "Epoch  41 Batch 1274/4686   train_loss = 1.814\n",
            "Epoch  41 Batch 1474/4686   train_loss = 1.959\n",
            "Epoch  41 Batch 1674/4686   train_loss = 1.677\n",
            "Epoch  41 Batch 1874/4686   train_loss = 2.083\n",
            "Epoch  41 Batch 2074/4686   train_loss = 1.853\n",
            "Epoch  41 Batch 2274/4686   train_loss = 1.795\n",
            "Epoch  41 Batch 2474/4686   train_loss = 1.770\n",
            "Epoch  41 Batch 2674/4686   train_loss = 1.907\n",
            "Epoch  41 Batch 2874/4686   train_loss = 1.974\n",
            "Epoch  41 Batch 3074/4686   train_loss = 1.945\n",
            "Epoch  41 Batch 3274/4686   train_loss = 1.754\n",
            "Epoch  41 Batch 3474/4686   train_loss = 1.658\n",
            "Epoch  41 Batch 3674/4686   train_loss = 1.791\n",
            "Epoch  41 Batch 3874/4686   train_loss = 2.088\n",
            "Epoch  41 Batch 4074/4686   train_loss = 1.748\n",
            "Epoch  41 Batch 4274/4686   train_loss = 1.738\n",
            "Epoch  41 Batch 4474/4686   train_loss = 1.853\n",
            "Epoch  41 Batch 4674/4686   train_loss = 1.872\n",
            "Epoch  42 Batch  188/4686   train_loss = 1.999\n",
            "Epoch  42 Batch  388/4686   train_loss = 2.101\n",
            "Epoch  42 Batch  588/4686   train_loss = 2.005\n",
            "Epoch  42 Batch  788/4686   train_loss = 1.924\n",
            "Epoch  42 Batch  988/4686   train_loss = 1.970\n",
            "Epoch  42 Batch 1188/4686   train_loss = 1.719\n",
            "Epoch  42 Batch 1388/4686   train_loss = 1.679\n",
            "Epoch  42 Batch 1588/4686   train_loss = 1.627\n",
            "Epoch  42 Batch 1788/4686   train_loss = 1.703\n",
            "Epoch  42 Batch 1988/4686   train_loss = 1.657\n",
            "Epoch  42 Batch 2188/4686   train_loss = 1.886\n",
            "Epoch  42 Batch 2388/4686   train_loss = 1.865\n",
            "Epoch  42 Batch 2588/4686   train_loss = 1.669\n",
            "Epoch  42 Batch 2788/4686   train_loss = 1.699\n",
            "Epoch  42 Batch 2988/4686   train_loss = 1.850\n",
            "Epoch  42 Batch 3188/4686   train_loss = 1.832\n",
            "Epoch  42 Batch 3388/4686   train_loss = 1.992\n",
            "Epoch  42 Batch 3588/4686   train_loss = 1.783\n",
            "Epoch  42 Batch 3788/4686   train_loss = 2.024\n",
            "Epoch  42 Batch 3988/4686   train_loss = 1.890\n",
            "Epoch  42 Batch 4188/4686   train_loss = 1.946\n",
            "Epoch  42 Batch 4388/4686   train_loss = 1.869\n",
            "Epoch  42 Batch 4588/4686   train_loss = 2.160\n",
            "Epoch  43 Batch  102/4686   train_loss = 1.822\n",
            "Epoch  43 Batch  302/4686   train_loss = 2.011\n",
            "Epoch  43 Batch  502/4686   train_loss = 1.765\n",
            "Epoch  43 Batch  702/4686   train_loss = 1.917\n",
            "Epoch  43 Batch  902/4686   train_loss = 1.904\n",
            "Epoch  43 Batch 1102/4686   train_loss = 1.922\n",
            "Epoch  43 Batch 1302/4686   train_loss = 1.804\n",
            "Epoch  43 Batch 1502/4686   train_loss = 1.806\n",
            "Epoch  43 Batch 1702/4686   train_loss = 1.792\n",
            "Epoch  43 Batch 1902/4686   train_loss = 2.054\n",
            "Epoch  43 Batch 2102/4686   train_loss = 1.881\n",
            "Epoch  43 Batch 2302/4686   train_loss = 1.974\n",
            "Epoch  43 Batch 2502/4686   train_loss = 1.691\n",
            "Epoch  43 Batch 2702/4686   train_loss = 1.769\n",
            "Epoch  43 Batch 2902/4686   train_loss = 1.841\n",
            "Epoch  43 Batch 3102/4686   train_loss = 1.572\n",
            "Epoch  43 Batch 3302/4686   train_loss = 1.528\n",
            "Epoch  43 Batch 3502/4686   train_loss = 1.773\n",
            "Epoch  43 Batch 3702/4686   train_loss = 1.956\n",
            "Epoch  43 Batch 3902/4686   train_loss = 1.684\n",
            "Epoch  43 Batch 4102/4686   train_loss = 1.886\n",
            "Epoch  43 Batch 4302/4686   train_loss = 1.691\n",
            "Epoch  43 Batch 4502/4686   train_loss = 1.836\n",
            "Epoch  44 Batch   16/4686   train_loss = 1.900\n",
            "Epoch  44 Batch  216/4686   train_loss = 1.675\n",
            "Epoch  44 Batch  416/4686   train_loss = 1.886\n",
            "Epoch  44 Batch  616/4686   train_loss = 1.831\n",
            "Epoch  44 Batch  816/4686   train_loss = 2.031\n",
            "Epoch  44 Batch 1016/4686   train_loss = 1.865\n",
            "Epoch  44 Batch 1216/4686   train_loss = 1.676\n",
            "Epoch  44 Batch 1416/4686   train_loss = 1.978\n",
            "Epoch  44 Batch 1616/4686   train_loss = 1.977\n",
            "Epoch  44 Batch 1816/4686   train_loss = 1.687\n",
            "Epoch  44 Batch 2016/4686   train_loss = 1.782\n",
            "Epoch  44 Batch 2216/4686   train_loss = 1.896\n",
            "Epoch  44 Batch 2416/4686   train_loss = 1.726\n",
            "Epoch  44 Batch 2616/4686   train_loss = 1.989\n",
            "Epoch  44 Batch 2816/4686   train_loss = 1.809\n",
            "Epoch  44 Batch 3016/4686   train_loss = 1.550\n",
            "Epoch  44 Batch 3216/4686   train_loss = 1.965\n",
            "Epoch  44 Batch 3416/4686   train_loss = 1.942\n",
            "Epoch  44 Batch 3616/4686   train_loss = 1.877\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch  44 Batch 3816/4686   train_loss = 1.947\n",
            "Epoch  44 Batch 4016/4686   train_loss = 1.765\n",
            "Epoch  44 Batch 4216/4686   train_loss = 1.855\n",
            "Epoch  44 Batch 4416/4686   train_loss = 1.974\n",
            "Epoch  44 Batch 4616/4686   train_loss = 1.850\n",
            "Epoch  45 Batch  130/4686   train_loss = 1.779\n",
            "Epoch  45 Batch  330/4686   train_loss = 2.046\n",
            "Epoch  45 Batch  530/4686   train_loss = 2.007\n",
            "Epoch  45 Batch  730/4686   train_loss = 1.737\n",
            "Epoch  45 Batch  930/4686   train_loss = 1.728\n",
            "Epoch  45 Batch 1130/4686   train_loss = 1.706\n",
            "Epoch  45 Batch 1330/4686   train_loss = 1.951\n",
            "Epoch  45 Batch 1530/4686   train_loss = 1.705\n",
            "Epoch  45 Batch 1730/4686   train_loss = 1.839\n",
            "Epoch  45 Batch 1930/4686   train_loss = 2.195\n",
            "Epoch  45 Batch 2130/4686   train_loss = 2.027\n",
            "Epoch  45 Batch 2330/4686   train_loss = 2.050\n",
            "Epoch  45 Batch 2530/4686   train_loss = 1.837\n",
            "Epoch  45 Batch 2730/4686   train_loss = 1.752\n",
            "Epoch  45 Batch 2930/4686   train_loss = 1.898\n",
            "Epoch  45 Batch 3130/4686   train_loss = 1.604\n",
            "Epoch  45 Batch 3330/4686   train_loss = 1.819\n",
            "Epoch  45 Batch 3530/4686   train_loss = 1.663\n",
            "Epoch  45 Batch 3730/4686   train_loss = 1.766\n",
            "Epoch  45 Batch 3930/4686   train_loss = 1.888\n",
            "Epoch  45 Batch 4130/4686   train_loss = 1.694\n",
            "Epoch  45 Batch 4330/4686   train_loss = 2.118\n",
            "Epoch  45 Batch 4530/4686   train_loss = 2.017\n",
            "Epoch  46 Batch   44/4686   train_loss = 1.762\n",
            "Epoch  46 Batch  244/4686   train_loss = 1.802\n",
            "Epoch  46 Batch  444/4686   train_loss = 1.823\n",
            "Epoch  46 Batch  644/4686   train_loss = 1.832\n",
            "Epoch  46 Batch  844/4686   train_loss = 1.647\n",
            "Epoch  46 Batch 1044/4686   train_loss = 1.842\n",
            "Epoch  46 Batch 1244/4686   train_loss = 1.834\n",
            "Epoch  46 Batch 1444/4686   train_loss = 1.789\n",
            "Epoch  46 Batch 1644/4686   train_loss = 1.890\n",
            "Epoch  46 Batch 1844/4686   train_loss = 1.920\n",
            "Epoch  46 Batch 2044/4686   train_loss = 1.975\n",
            "Epoch  46 Batch 2244/4686   train_loss = 1.765\n",
            "Epoch  46 Batch 2444/4686   train_loss = 1.516\n",
            "Epoch  46 Batch 2644/4686   train_loss = 1.944\n",
            "Epoch  46 Batch 2844/4686   train_loss = 1.648\n",
            "Epoch  46 Batch 3044/4686   train_loss = 1.976\n",
            "Epoch  46 Batch 3244/4686   train_loss = 1.493\n",
            "Epoch  46 Batch 3444/4686   train_loss = 1.845\n",
            "Epoch  46 Batch 3644/4686   train_loss = 1.970\n",
            "Epoch  46 Batch 3844/4686   train_loss = 1.944\n",
            "Epoch  46 Batch 4044/4686   train_loss = 1.813\n",
            "Epoch  46 Batch 4244/4686   train_loss = 2.010\n",
            "Epoch  46 Batch 4444/4686   train_loss = 2.047\n",
            "Epoch  46 Batch 4644/4686   train_loss = 1.575\n",
            "Epoch  47 Batch  158/4686   train_loss = 1.708\n",
            "Epoch  47 Batch  358/4686   train_loss = 1.701\n",
            "Epoch  47 Batch  558/4686   train_loss = 1.610\n",
            "Epoch  47 Batch  758/4686   train_loss = 1.702\n",
            "Epoch  47 Batch  958/4686   train_loss = 1.976\n",
            "Epoch  47 Batch 1158/4686   train_loss = 1.656\n",
            "Epoch  47 Batch 1358/4686   train_loss = 1.982\n",
            "Epoch  47 Batch 1558/4686   train_loss = 1.836\n",
            "Epoch  47 Batch 1758/4686   train_loss = 1.969\n",
            "Epoch  47 Batch 1958/4686   train_loss = 1.806\n",
            "Epoch  47 Batch 2158/4686   train_loss = 1.910\n",
            "Epoch  47 Batch 2358/4686   train_loss = 1.942\n",
            "Epoch  47 Batch 2558/4686   train_loss = 1.535\n",
            "Epoch  47 Batch 2758/4686   train_loss = 1.951\n",
            "Epoch  47 Batch 2958/4686   train_loss = 1.936\n",
            "Epoch  47 Batch 3158/4686   train_loss = 1.871\n",
            "Epoch  47 Batch 3358/4686   train_loss = 1.956\n",
            "Epoch  47 Batch 3558/4686   train_loss = 1.888\n",
            "Epoch  47 Batch 3758/4686   train_loss = 1.901\n",
            "Epoch  47 Batch 3958/4686   train_loss = 1.821\n",
            "Epoch  47 Batch 4158/4686   train_loss = 1.842\n",
            "Epoch  47 Batch 4358/4686   train_loss = 1.695\n",
            "Epoch  47 Batch 4558/4686   train_loss = 1.762\n",
            "Epoch  48 Batch   72/4686   train_loss = 1.743\n",
            "Epoch  48 Batch  272/4686   train_loss = 1.947\n",
            "Epoch  48 Batch  472/4686   train_loss = 1.680\n",
            "Epoch  48 Batch  672/4686   train_loss = 1.771\n",
            "Epoch  48 Batch  872/4686   train_loss = 1.837\n",
            "Epoch  48 Batch 1072/4686   train_loss = 2.011\n",
            "Epoch  48 Batch 1272/4686   train_loss = 1.597\n",
            "Epoch  48 Batch 1472/4686   train_loss = 1.593\n",
            "Epoch  48 Batch 1672/4686   train_loss = 1.869\n",
            "Epoch  48 Batch 1872/4686   train_loss = 1.832\n",
            "Epoch  48 Batch 2072/4686   train_loss = 1.811\n",
            "Epoch  48 Batch 2272/4686   train_loss = 1.748\n",
            "Epoch  48 Batch 2472/4686   train_loss = 1.853\n",
            "Epoch  48 Batch 2672/4686   train_loss = 1.748\n",
            "Epoch  48 Batch 2872/4686   train_loss = 1.754\n",
            "Epoch  48 Batch 3072/4686   train_loss = 1.844\n",
            "Epoch  48 Batch 3272/4686   train_loss = 1.929\n",
            "Epoch  48 Batch 3472/4686   train_loss = 1.857\n",
            "Epoch  48 Batch 3672/4686   train_loss = 1.867\n",
            "Epoch  48 Batch 3872/4686   train_loss = 1.951\n",
            "Epoch  48 Batch 4072/4686   train_loss = 1.987\n",
            "Epoch  48 Batch 4272/4686   train_loss = 1.785\n",
            "Epoch  48 Batch 4472/4686   train_loss = 1.779\n",
            "Epoch  48 Batch 4672/4686   train_loss = 1.747\n",
            "Epoch  49 Batch  186/4686   train_loss = 1.738\n",
            "Epoch  49 Batch  386/4686   train_loss = 1.606\n",
            "Epoch  49 Batch  586/4686   train_loss = 1.726\n",
            "Epoch  49 Batch  786/4686   train_loss = 1.741\n",
            "Epoch  49 Batch  986/4686   train_loss = 1.718\n",
            "Epoch  49 Batch 1186/4686   train_loss = 1.737\n",
            "Epoch  49 Batch 1386/4686   train_loss = 1.839\n",
            "Epoch  49 Batch 1586/4686   train_loss = 2.050\n",
            "Epoch  49 Batch 1786/4686   train_loss = 1.798\n",
            "Epoch  49 Batch 1986/4686   train_loss = 1.751\n",
            "Epoch  49 Batch 2186/4686   train_loss = 1.680\n",
            "Epoch  49 Batch 2386/4686   train_loss = 1.641\n",
            "Epoch  49 Batch 2586/4686   train_loss = 1.912\n",
            "Epoch  49 Batch 2786/4686   train_loss = 1.811\n",
            "Epoch  49 Batch 2986/4686   train_loss = 1.949\n",
            "Epoch  49 Batch 3186/4686   train_loss = 1.821\n",
            "Epoch  49 Batch 3386/4686   train_loss = 1.664\n",
            "Epoch  49 Batch 3586/4686   train_loss = 1.735\n",
            "Epoch  49 Batch 3786/4686   train_loss = 2.175\n",
            "Epoch  49 Batch 3986/4686   train_loss = 1.710\n",
            "Epoch  49 Batch 4186/4686   train_loss = 1.969\n",
            "Epoch  49 Batch 4386/4686   train_loss = 2.055\n",
            "Epoch  49 Batch 4586/4686   train_loss = 1.862\n",
            "Model Trained and Saved\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1meu67diZUuh",
        "colab_type": "text"
      },
      "source": [
        "### Generate TV Script\n",
        "When training is finished we are at the last step of this project: generating a new TV Script for \"The Simpsons\"!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_0BTeYLZUui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_tensors(loaded_graph):\n",
        "    input_tensor = loaded_graph.get_tensor_by_name('input:0')\n",
        "    initial_state_tensor = loaded_graph.get_tensor_by_name('initial_state:0')\n",
        "    final_state_tensor = loaded_graph.get_tensor_by_name('final_state:0')\n",
        "    probs_tensor = loaded_graph.get_tensor_by_name('probs:0')\n",
        "    return input_tensor, initial_state_tensor, final_state_tensor, probs_tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyZ6PUxhZUul",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pick_word(probabilities, int_to_vocab):\n",
        "    word_id = np.argmax(probabilities)\n",
        "    word_string = int_to_vocab[word_id]\n",
        "    return word_string"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSzu-6Y6ZUuo",
        "colab_type": "code",
        "colab": {},
        "outputId": "763f05c9-14d3-4a87-f551-b5e2da729369"
      },
      "source": [
        "gen_length = 500\n",
        "\n",
        "prime_word = 'homer_simpson'\n",
        "\n",
        "loaded_graph = tf.Graph()\n",
        "with tf.Session(graph=loaded_graph) as sess:\n",
        "    # Load saved model\n",
        "    loader = tf.train.import_meta_graph(save_dir + '.meta')\n",
        "    loader.restore(sess, save_dir)\n",
        "\n",
        "    # Get Tensors from loaded model\n",
        "    input_text, initial_state, final_state, probs = get_tensors(loaded_graph)\n",
        "\n",
        "    # Sentences generation setup\n",
        "    gen_sentences = [prime_word + ':']\n",
        "    prev_state = sess.run(initial_state, {input_text: np.array([[1]])})\n",
        "\n",
        "    # Generate sentences\n",
        "    for n in range(gen_length):\n",
        "        # Dynamic Input\n",
        "        dyn_input = [[vocab_to_int[word] for word in gen_sentences[-seq_length:]]]\n",
        "        dyn_seq_length = len(dyn_input[0])\n",
        "\n",
        "        # Get Prediction\n",
        "        probabilities, prev_state = sess.run(\n",
        "            [probs, final_state],\n",
        "            {input_text: dyn_input, initial_state: prev_state})\n",
        "        \n",
        "        pred_word = pick_word(probabilities[0][dyn_seq_length-1], int_to_vocab)\n",
        "\n",
        "        gen_sentences.append(pred_word)\n",
        "    \n",
        "    # Remove tokens\n",
        "    tv_script = ' '.join(gen_sentences)\n",
        "    for key, token in tokenized_punctuation.items():\n",
        "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
        "        tv_script = tv_script.replace(' ' + token.lower(), key)\n",
        "    tv_script = tv_script.replace('\\n ', '\\n')\n",
        "    tv_script = tv_script.replace('( ', '(')\n",
        "        \n",
        "    print(tv_script)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./save\n",
            "homer_simpson:(moans)\n",
            "marge_simpson:(annoyed murmur)\n",
            "homer_simpson:(annoyed grunt)\n",
            "(moe's_tavern: ext. moe's - night)\n",
            "homer_simpson:(to moe) this is a great idea, children. now, what are we playing here?\n",
            "bart_simpson:(horrified gasp)\n",
            "(simpson_home: ext. simpson house - day - establishing)\n",
            "homer_simpson:(worried) i've got a wet!\n",
            "homer_simpson:(faking enthusiasm) well, maybe i could kiss my little girl. mine!\n",
            "(department int. sports arena - night)\n",
            "seymour_skinner:(chuckles)\n",
            "chief_wiggum:(laughing) oh, i get it.\n",
            "seymour_skinner:(snapping) i guess this building is quiet.\n",
            "homer_simpson:(stunned) what? how'd you like that?\n",
            "professor_jonathan_frink: uh, well, looks like the little bit of you.\n",
            "bart_simpson:(to larry) i guess this is clearly justin, right?\n",
            "homer_simpson:(dismissive snort) oh, i am.\n",
            "marge_simpson:(pained) hi.\n",
            "homer_simpson:(pained sound) i thought you might have some good choice.\n",
            "homer_simpson:(pained) oh, sorry.\n",
            "(simpson_home: int. simpson house - living room - day)\n",
            "marge_simpson:(concerned) okay, open your door.\n",
            "homer_simpson: don't push, marge. we'll be fine.\n",
            "judge_snyder:(sarcastic) children, you want a night?\n",
            "homer_simpson:(gulp) oh, i can't believe i wasn't in a car.\n",
            "chief_wiggum:(to selma) i can't find this map. and she's gonna release that?\n",
            "homer_simpson:(lots of hair) just like me.\n",
            "homer_simpson:(shrugs) gimme a try.\n",
            "homer_simpson:(sweetly) i don't know, but i don't remember that.\n",
            "marge_simpson:(brightening) are you all right?\n",
            "homer_simpson: absolutely...\n",
            "lisa_simpson:(mad) even better!\n",
            "homer_simpson:(hums)\n",
            "marge_simpson: oh, homie. that's a doggie door.\n",
            "homer_simpson:(moan) i don't have computers.\n",
            "homer_simpson:(hopeful) honey?(makes fake companies break) are you okay?\n",
            "marge_simpson:(short giggle)\n",
            "homer_simpson:(happy) oh, marge, i found the two thousand and cars.\n",
            "marge_simpson:(frustrated sound)\n",
            "lisa_simpson:(outraged) are you, you're too far to go?\n",
            "boys:(skeptical) well, i'm gonna be here at the same time.\n",
            "homer_simpson:(moans) why are you doing us by doing anything?\n",
            "marge_simpson: well, it always seemed like i'm gonna be friends with...\n",
            "homer_simpson:(seething) losers!\n",
            "(simpson_home: int. simpson house -\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdRiSLwJZUuu",
        "colab_type": "text"
      },
      "source": [
        "### Conclusion\n",
        "We have trained a model to generate new text!\n",
        "\n",
        "As you can see the text does not really make any sense, but that's ok. This project was meant to show you how to prepare the data for training the model and to give a basic idea on how NLG works.\n",
        "\n",
        "If you want you can tune the parameters, add more layers or change their size. Look at how the output of the model changes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MqXO3_4ZUuv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}